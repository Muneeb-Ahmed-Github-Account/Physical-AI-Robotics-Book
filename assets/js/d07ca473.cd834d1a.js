"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[7096],{5214:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"nvidia-isaac/examples","title":"Isaac Code Examples and Applications","description":"Practical code examples and applications using NVIDIA Isaac for humanoid robotics","source":"@site/docs/nvidia-isaac/examples.md","sourceDirName":"nvidia-isaac","slug":"/nvidia-isaac/examples","permalink":"/Physical-AI-Robotics-Book/docs/nvidia-isaac/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/Muneeb-Ahmed-Github-Account/Physical-AI-Robotics-Book/tree/main/docs/nvidia-isaac/examples.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Isaac Code Examples and Applications","sidebar_position":4,"description":"Practical code examples and applications using NVIDIA Isaac for humanoid robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Architecture and Core Concepts","permalink":"/Physical-AI-Robotics-Book/docs/nvidia-isaac/core-concepts"},"next":{"title":"Isaac Development Best Practices","permalink":"/Physical-AI-Robotics-Book/docs/nvidia-isaac/best-practices"}}');var i=a(4848),t=a(8453);const r={title:"Isaac Code Examples and Applications",sidebar_position:4,description:"Practical code examples and applications using NVIDIA Isaac for humanoid robotics"},o="Isaac Code Examples and Applications",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Isaac Sim Examples",id:"isaac-sim-examples",level:2},{value:"Basic Humanoid Robot Simulation",id:"basic-humanoid-robot-simulation",level:3},{value:"Loading and Configuring a Humanoid Model",id:"loading-and-configuring-a-humanoid-model",level:4},{value:"Isaac ROS Perception Pipeline Example",id:"isaac-ros-perception-pipeline-example",level:3},{value:"GPU-Accelerated Image Processing",id:"gpu-accelerated-image-processing",level:4},{value:"Isaac Visual SLAM Example",id:"isaac-visual-slam-example",level:3},{value:"Isaac Apps Examples",id:"isaac-apps-examples",level:2},{value:"Isaac Navigation Example",id:"isaac-navigation-example",level:3},{value:"Isaac Manipulation Example",id:"isaac-manipulation-example",level:2},{value:"GPU-Accelerated Grasp Planning",id:"gpu-accelerated-grasp-planning",level:3},{value:"Isaac Integration Examples",id:"isaac-integration-examples",level:2},{value:"Isaac Sim to Real Robot Transfer",id:"isaac-sim-to-real-robot-transfer",level:3},{value:"Isaac AI Integration Example",id:"isaac-ai-integration-example",level:2},{value:"GPU-Accelerated Perception with Isaac",id:"gpu-accelerated-perception-with-isaac",level:3},{value:"Isaac Launch Files",id:"isaac-launch-files",level:2},{value:"Isaac Navigation Launch Example",id:"isaac-navigation-launch-example",level:3},{value:"Isaac Configuration Files",id:"isaac-configuration-files",level:2},{value:"Isaac Parameter Configuration",id:"isaac-parameter-configuration",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"isaac-code-examples-and-applications",children:"Isaac Code Examples and Applications"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this section, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement GPU-accelerated perception pipelines using Isaac ROS [1]"}),"\n",(0,i.jsx)(n.li,{children:"Create simulation environments for humanoid robot testing [2]"}),"\n",(0,i.jsx)(n.li,{children:"Integrate Isaac Sim with real robot hardware [3]"}),"\n",(0,i.jsx)(n.li,{children:"Deploy Isaac-based applications to humanoid robots [4]"}),"\n",(0,i.jsx)(n.li,{children:"Optimize Isaac applications for real-time performance [5]"}),"\n",(0,i.jsx)(n.li,{children:"Apply Isaac tools for sim-to-real transfer [6]"}),"\n",(0,i.jsx)(n.li,{children:"Implement Isaac-based AI integration for humanoid systems [7]"}),"\n",(0,i.jsx)(n.li,{children:"Configure Isaac for multi-robot coordination [8]"}),"\n",(0,i.jsx)(n.li,{children:"Validate Isaac applications with comprehensive testing [9]"}),"\n",(0,i.jsx)(n.li,{children:"Troubleshoot common Isaac application issues [10]"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sim-examples",children:"Isaac Sim Examples"}),"\n",(0,i.jsx)(n.h3,{id:"basic-humanoid-robot-simulation",children:"Basic Humanoid Robot Simulation"}),"\n",(0,i.jsx)(n.h4,{id:"loading-and-configuring-a-humanoid-model",children:"Loading and Configuring a Humanoid Model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Loading a humanoid robot model in Isaac Sim\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.utils.viewports import set_camera_view\nimport numpy as np\n\nclass HumanoidSimulationExample:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Load humanoid robot model (using a generic model for example)\n        self.robot_path = "/World/HumanoidRobot"\n\n        # Add robot to stage (in a real implementation, you would use your specific robot model)\n        add_reference_to_stage(\n            usd_path="path/to/your/humanoid/model.usd",  # Replace with actual path\n            prim_path=self.robot_path\n        )\n\n        # Create articulation for the robot\n        self.robot = self.world.scene.add(\n            Articulation(\n                prim_path=self.robot_path,\n                name="humanoid_robot"\n            )\n        )\n\n        # Set up camera view\n        set_camera_view(eye=np.array([2.0, 2.0, 2.0]),\n                       target=np.array([0.0, 0.0, 1.0]))\n\n    def setup_physics(self):\n        """Configure physics properties for humanoid simulation"""\n        # Set gravity\n        self.world.scene.set_gravity([0.0, 0.0, -9.81])\n\n        # Configure physics solver parameters for humanoid stability\n        physics_ctx = self.world.physics_sim_view\n        physics_ctx.set_solver_type(0)  # TGS solver for better stability\n        physics_ctx.set_position_iteration_count(8)\n        physics_ctx.set_velocity_iteration_count(2)\n\n        # Enable continuous collision detection for fast-moving parts\n        physics_ctx.enable_ccd(True)\n\n    def run_simulation(self):\n        """Run the simulation with humanoid control"""\n        self.world.reset()\n\n        # Initialize physics\n        self.setup_physics()\n\n        # Simulation loop\n        for i in range(1000):  # Run for 1000 steps\n            # Reset every 100 steps to demonstrate control\n            if i % 100 == 0:\n                self.world.reset()\n\n                # Apply initial configuration\n                joint_positions = [0.0] * self.robot.num_dof\n                self.robot.set_joint_positions(np.array(joint_positions))\n\n            # Step the world\n            self.world.step(render=True)\n\n            # Print robot state periodically\n            if i % 50 == 0:\n                joint_positions = self.robot.get_joint_positions()\n                joint_velocities = self.robot.get_joint_velocities()\n                print(f"Step {i}: Joint positions: {joint_positions[:3]}...")  # Print first 3 joints\n\n    def cleanup(self):\n        """Clean up the simulation"""\n        self.world.clear()\n\ndef main():\n    sim = HumanoidSimulationExample()\n    try:\n        sim.run_simulation()\n    finally:\n        sim.cleanup()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-perception-pipeline-example",children:"Isaac ROS Perception Pipeline Example"}),"\n",(0,i.jsx)(n.h4,{id:"gpu-accelerated-image-processing",children:"GPU-Accelerated Image Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Isaac ROS image processing pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport time\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_perception_pipeline\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/front_camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            PointStamped,\n            \'/object_detection/result\',\n            10\n        )\n\n        # Isaac-specific GPU-accelerated processing parameters\n        self.declare_parameter(\'use_gpu_acceleration\', True)\n        self.use_gpu = self.get_parameter(\'use_gpu_acceleration\').value\n\n        # Initialize Isaac perception components\n        self.initialize_isaac_perception()\n\n        self.get_logger().info(\'Isaac Perception Pipeline initialized\')\n\n    def initialize_isaac_perception(self):\n        """Initialize Isaac-specific perception components"""\n        if self.use_gpu:\n            try:\n                import pycuda.driver as cuda\n                import pycuda.autoinit\n                from pycuda.compiler import SourceModule\n\n                # Initialize CUDA context for Isaac acceleration\n                self.cuda_ctx = cuda.Device(0).make_context()\n\n                # Compile Isaac-specific CUDA kernels for image processing\n                self.compile_isaac_kernels()\n\n                self.get_logger().info(\'Isaac GPU acceleration enabled\')\n            except ImportError:\n                self.get_logger().warn(\'CUDA not available, using CPU processing\')\n                self.use_gpu = False\n        else:\n            self.get_logger().info(\'Using CPU-based processing\')\n\n    def compile_isaac_kernels(self):\n        """Compile Isaac-specific CUDA kernels"""\n        # Example CUDA kernel for Isaac GPU-accelerated image processing\n        cuda_code = """\n        __global__ void detect_features_kernel(\n            float* input_image,\n            float* output_features,\n            int width,\n            int height\n        ) {\n            int x = blockIdx.x * blockDim.x + threadIdx.x;\n            int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n            if (x < width && y < height) {\n                int idx = y * width + x;\n\n                // Simple edge detection for example\n                if (x > 0 && x < width-1 && y > 0 && y < height-1) {\n                    float center = input_image[idx];\n                    float left = input_image[y * width + (x-1)];\n                    float right = input_image[y * width + (x+1)];\n                    float top = input_image[(y-1) * width + x];\n                    float bottom = input_image[(y+1) * width + x];\n\n                    float gradient = fabsf(center - left) + fabsf(center - right) +\n                                   fabsf(center - top) + fabsf(center - bottom);\n\n                    output_features[idx] = gradient > 0.1f ? 1.0f : 0.0f;\n                } else {\n                    output_features[idx] = 0.0f;\n                }\n            }\n        }\n        """\n\n        self.feature_module = SourceModule(cuda_code)\n        self.feature_kernel = self.feature_module.get_function("detect_features_kernel")\n\n    def image_callback(self, msg: Image):\n        """Process incoming image with Isaac acceleration"""\n        start_time = time.time()\n\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\n\n            # Normalize image to float32 for processing\n            if cv_image.dtype == np.uint8:\n                cv_image = cv_image.astype(np.float32) / 255.0\n\n            # Apply Isaac-accelerated processing\n            if self.use_gpu and hasattr(self, \'feature_kernel\'):\n                processed_features = self.isaac_feature_detection_gpu(cv_image)\n            else:\n                processed_features = self.cpu_feature_detection(cv_image)\n\n            # Process results and publish\n            result_point = self.extract_keypoint(processed_features, cv_image.shape)\n\n            if result_point is not None:\n                # Create and publish result\n                detection_msg = PointStamped()\n                detection_msg.header = Header()\n                detection_msg.header.stamp = self.get_clock().now().to_msg()\n                detection_msg.header.frame_id = msg.header.frame_id\n                detection_msg.point.x = result_point[0]\n                detection_msg.point.y = result_point[1]\n                detection_msg.point.z = result_point[2]\n\n                self.detection_pub.publish(detection_msg)\n\n                self.get_logger().info(f\'Detected feature at: ({result_point[0]:.2f}, {result_point[1]:.2f}, {result_point[2]:.2f})\')\n\n            processing_time = (time.time() - start_time) * 1000\n            self.get_logger().info(f\'Processing time: {processing_time:.2f}ms\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def isaac_feature_detection_gpu(self, image):\n        """GPU-accelerated feature detection using Isaac CUDA"""\n        height, width = image.shape[:2]\n        channels = 1 if len(image.shape) == 2 else image.shape[2]\n\n        # Flatten image for GPU processing\n        flat_image = image.flatten().astype(np.float32)\n\n        # Allocate GPU memory\n        input_gpu = cuda.mem_alloc(flat_image.nbytes)\n        output_gpu = cuda.mem_alloc(flat_image.nbytes)\n\n        # Copy image to GPU\n        cuda.memcpy_htod(input_gpu, flat_image)\n\n        # Configure kernel execution\n        block_size = (16, 16, 1)\n        grid_size = ((width + block_size[0] - 1) // block_size[0],\n                     (height + block_size[1] - 1) // block_size[1], 1)\n\n        # Execute kernel\n        self.feature_kernel(\n            input_gpu, output_gpu, np.int32(width), np.int32(height),\n            block=block_size, grid=grid_size\n        )\n\n        # Copy result back to CPU\n        result_flat = np.empty_like(flat_image)\n        cuda.memcpy_dtoh(result_flat, output_gpu)\n\n        # Clean up GPU memory\n        del input_gpu\n        del output_gpu\n\n        # Reshape result\n        return result_flat.reshape((height, width))\n\n    def cpu_feature_detection(self, image):\n        """CPU-based feature detection as fallback"""\n        # Simple CPU implementation of feature detection\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n        edges = cv2.Canny((gray * 255).astype(np.uint8), 50, 150)\n        return edges.astype(np.float32) / 255.0\n\n    def extract_keypoint(self, features, image_shape):\n        """Extract keypoint from detected features"""\n        height, width = image_shape[:2]\n\n        # Find strongest feature response\n        max_idx = np.argmax(features)\n        max_y, max_x = np.unravel_index(max_idx, features.shape)\n\n        # Convert to normalized coordinates\n        norm_x = (max_x / width) * 2 - 1  # Normalize to [-1, 1]\n        norm_y = (max_y / height) * 2 - 1  # Normalize to [-1, 1]\n        norm_z = 0.0  # Assume feature is on image plane\n\n        return (norm_x, norm_y, norm_z)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    perception_pipeline = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_pipeline)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-visual-slam-example",children:"Isaac Visual SLAM Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example: Isaac Visual SLAM application\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom tf2_ros import TransformBroadcaster\nimport numpy as np\n\nclass IsaacVisualSLAM(Node):\n    def __init__(self):\n        super().__init__('isaac_visual_slam')\n\n        # Isaac-specific SLAM parameters\n        self.declare_parameter('enable_gpu_acceleration', True)\n        self.declare_parameter('max_features', 2000)\n        self.declare_parameter('min_match_distance', 30.0)\n        self.declare_parameter('keyframe_threshold', 0.1)\n\n        # Initialize GPU acceleration if available\n        self.use_gpu = self.get_parameter('enable_gpu_acceleration').value\n        self.initialize_gpu_acceleration()\n\n        # Create subscribers for stereo camera\n        self.left_image_sub = self.create_subscription(\n            Image, '/stereo_camera/left/image_rect_color',\n            self.left_image_callback, 10\n        )\n\n        self.right_image_sub = self.create_subscription(\n            Image, '/stereo_camera/right/image_rect_color',\n            self.right_image_callback, 10\n        )\n\n        self.left_info_sub = self.create_subscription(\n            CameraInfo, '/stereo_camera/left/camera_info',\n            self.left_info_callback, 10\n        )\n\n        # Create publishers\n        self.odom_pub = self.create_publisher(Odometry, '/visual_odom', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, '/visual_pose', 10)\n\n        # Initialize TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # SLAM state variables\n        self.prev_features = None\n        self.prev_pose = np.eye(4)\n        self.current_pose = np.eye(4)\n\n        self.get_logger().info('Isaac Visual SLAM initialized')\n\n    def initialize_gpu_acceleration(self):\n        \"\"\"Initialize GPU acceleration for visual SLAM\"\"\"\n        if self.use_gpu:\n            try:\n                import pycuda.driver as cuda\n                import pycuda.autoinit\n                import skcuda.linalg as culinalg\n                import skcuda.misc as cumisc\n\n                self.cuda_available = True\n                self.get_logger().info('Isaac GPU acceleration enabled for SLAM')\n            except ImportError:\n                self.cuda_available = False\n                self.get_logger().warn('CUDA not available for SLAM, using CPU fallback')\n        else:\n            self.cuda_available = False\n\n    def left_image_callback(self, msg: Image):\n        \"\"\"Process left camera image for visual SLAM\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_bridge = CvBridge()\n            cv_image = cv_bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n            # Extract features using Isaac-accelerated methods\n            if self.cuda_available:\n                keypoints, descriptors = self.extract_features_gpu(cv_image)\n            else:\n                keypoints, descriptors = self.extract_features_cpu(cv_image)\n\n            # Process SLAM if we have previous features\n            if self.prev_features is not None:\n                transformation = self.estimate_motion(\n                    self.prev_features['descriptors'],\n                    descriptors,\n                    self.prev_features['keypoints'],\n                    keypoints\n                )\n\n                if transformation is not None:\n                    # Update pose\n                    self.current_pose = self.current_pose @ transformation\n\n                    # Publish odometry\n                    self.publish_odometry(msg.header.stamp, msg.header.frame_id)\n\n                    # Check if we should create a keyframe\n                    translation_norm = np.linalg.norm(transformation[:3, 3])\n                    if translation_norm > self.get_parameter('keyframe_threshold').value:\n                        self.prev_features = {\n                            'keypoints': keypoints,\n                            'descriptors': descriptors\n                        }\n\n            else:\n                # Store initial features\n                self.prev_features = {\n                    'keypoints': keypoints,\n                    'descriptors': descriptors\n                }\n\n        except Exception as e:\n            self.get_logger().error(f'Error in SLAM processing: {e}')\n\n    def right_image_callback(self, msg: Image):\n        \"\"\"Process right camera image (for stereo depth estimation)\"\"\"\n        # In a complete implementation, this would be used for stereo processing\n        pass\n\n    def extract_features_gpu(self, image):\n        \"\"\"GPU-accelerated feature extraction\"\"\"\n        # This would use Isaac's GPU-accelerated feature extraction\n        # For this example, we'll use a CPU fallback with a note about GPU acceleration\n        self.get_logger().warn('GPU feature extraction not fully implemented in this example')\n        return self.extract_features_cpu(image)\n\n    def extract_features_cpu(self, image):\n        \"\"\"CPU-based feature extraction as fallback\"\"\"\n        try:\n            import cv2\n            # Use ORB as an example feature detector\n            orb = cv2.ORB_create(nfeatures=self.get_parameter('max_features').value)\n            keypoints, descriptors = orb.detectAndCompute(image, None)\n\n            if descriptors is not None:\n                # Normalize descriptors for matching\n                descriptors = descriptors.astype(np.float32)\n\n            return keypoints or [], descriptors\n        except ImportError:\n            self.get_logger().error('OpenCV not available for feature extraction')\n            return [], None\n\n    def estimate_motion(self, prev_desc, curr_desc, prev_kp, curr_kp):\n        \"\"\"Estimate motion between two frames\"\"\"\n        try:\n            import cv2\n            if prev_desc is None or curr_desc is None:\n                return None\n\n            # Use FLANN matcher for efficient matching\n            FLANN_INDEX_LSH = 6\n            index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6,\n                               key_size=12, multi_probe_level=1)\n            search_params = dict(checks=50)\n\n            flann = cv2.FlannBasedMatcher(index_params, search_params)\n            matches = flann.knnMatch(prev_desc, curr_desc, k=2)\n\n            # Apply Lowe's ratio test\n            good_matches = []\n            for m, n in matches:\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n\n            # Require minimum number of good matches\n            min_matches = 10\n            if len(good_matches) < min_matches:\n                self.get_logger().warn(f'Insufficient matches: {len(good_matches)} < {min_matches}')\n                return None\n\n            # Extract matched points\n            prev_pts = np.float32([prev_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            curr_pts = np.float32([curr_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n            # Estimate essential matrix and decompose to get rotation/translation\n            E, mask = cv2.findEssentialMat(curr_pts, prev_pts, focal=500.0, pp=(320, 240))\n            if E is not None:\n                _, R, t, _ = cv2.recoverPose(E, curr_pts, prev_pts)\n\n                # Create transformation matrix\n                transformation = np.eye(4)\n                transformation[:3, :3] = R\n                transformation[:3, 3] = t.flatten()\n\n                return transformation\n            else:\n                return None\n\n        except ImportError:\n            self.get_logger().error('OpenCV not available for motion estimation')\n            return None\n\n    def publish_odometry(self, stamp, frame_id):\n        \"\"\"Publish odometry information\"\"\"\n        odom_msg = Odometry()\n        odom_msg.header.stamp = stamp\n        odom_msg.header.frame_id = frame_id\n        odom_msg.child_frame_id = 'base_link'\n\n        # Set pose\n        odom_msg.pose.pose.position.x = self.current_pose[0, 3]\n        odom_msg.pose.pose.position.y = self.current_pose[1, 3]\n        odom_msg.pose.pose.position.z = self.current_pose[2, 3]\n\n        # Convert rotation matrix to quaternion\n        from scipy.spatial.transform import Rotation as R\n        r = R.from_matrix(self.current_pose[:3, :3])\n        quat = r.as_quat()\n        odom_msg.pose.pose.orientation.x = quat[0]\n        odom_msg.pose.pose.orientation.y = quat[1]\n        odom_msg.pose.pose.orientation.z = quat[2]\n        odom_msg.pose.pose.orientation.w = quat[3]\n\n        # Set twist (velocity estimation would go here)\n        odom_msg.twist.twist.linear.x = 0.0\n        odom_msg.twist.twist.angular.z = 0.0\n\n        self.odom_pub.publish(odom_msg)\n\n        # Broadcast transform\n        from geometry_msgs.msg import TransformStamped\n        t = TransformStamped()\n        t.header.stamp = stamp\n        t.header.frame_id = frame_id\n        t.child_frame_id = 'base_link'\n\n        t.transform.translation.x = self.current_pose[0, 3]\n        t.transform.translation.y = self.current_pose[1, 3]\n        t.transform.translation.z = self.current_pose[2, 3]\n\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    visual_slam = IsaacVisualSLAM()\n\n    try:\n        rclpy.spin(visual_slam)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        visual_slam.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-apps-examples",children:"Isaac Apps Examples"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-navigation-example",children:"Isaac Navigation Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Isaac Navigation application for humanoid robots\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Path, OccupancyGrid\nfrom sensor_msgs.msg import LaserScan\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\n\nclass IsaacHumanoidNavigator(Node):\n    def __init__(self):\n        super().__init__(\'isaac_humanoid_navigator\')\n\n        # Isaac Navigation parameters\n        self.declare_parameter(\'planner_frequency\', 10.0)\n        self.declare_parameter(\'controller_frequency\', 50.0)\n        self.declare_parameter(\'max_linear_speed\', 0.5)\n        self.declare_parameter(\'max_angular_speed\', 1.0)\n        self.declare_parameter(\'goal_tolerance\', 0.2)\n        self.declare_parameter(\'yaw_goal_tolerance\', 0.1)\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.path_pub = self.create_publisher(Path, \'/plan\', 10)\n\n        # Subscribers\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'/move_base_simple/goal\',\n            self.goal_callback, 10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\',\n            self.scan_callback, 10\n        )\n\n        self.map_sub = self.create_subscription(\n            OccupancyGrid, \'/map\',\n            self.map_callback, 10\n        )\n\n        # Navigation state\n        self.current_goal = None\n        self.current_plan = None\n        self.current_pose = None\n        self.obstacles = []\n\n        # Timers for planning and control\n        self.planner_timer = self.create_timer(\n            1.0 / self.get_parameter(\'planner_frequency\').value,\n            self.plan_callback\n        )\n\n        self.controller_timer = self.create_timer(\n            1.0 / self.get_parameter(\'controller_frequency\').value,\n            self.control_callback\n        )\n\n        self.get_logger().info(\'Isaac Humanoid Navigator initialized\')\n\n    def goal_callback(self, msg: PoseStamped):\n        """Handle new navigation goal"""\n        self.current_goal = msg\n        self.get_logger().info(f\'New goal received: ({msg.pose.position.x}, {msg.pose.position.y})\')\n\n        # Trigger immediate replanning\n        self.replan()\n\n    def scan_callback(self, msg: LaserScan):\n        """Process laser scan data for obstacle detection"""\n        # Convert laser scan to obstacle positions\n        angles = np.linspace(\n            msg.angle_min, msg.angle_max, len(msg.ranges)\n        )\n\n        valid_ranges = []\n        for i, range_val in enumerate(msg.ranges):\n            if msg.range_min <= range_val <= msg.range_max:\n                angle = angles[i]\n                x = range_val * np.cos(angle)\n                y = range_val * np.sin(angle)\n                valid_ranges.append((x, y))\n\n        self.obstacles = valid_ranges\n\n    def map_callback(self, msg: OccupancyGrid):\n        """Process occupancy grid map"""\n        # Store map information for path planning\n        self.map_resolution = msg.info.resolution\n        self.map_origin = (msg.info.origin.position.x, msg.info.origin.position.y)\n        self.map_data = np.array(msg.data).reshape(msg.info.height, msg.info.width)\n\n    def replan(self):\n        """Replan path to current goal"""\n        if self.current_goal is None or self.current_pose is None:\n            return\n\n        # In a real implementation, this would use Isaac\'s GPU-accelerated planners\n        # For this example, we\'ll use a simple approach\n        start = (self.current_pose.position.x, self.current_pose.position.y)\n        goal = (self.current_goal.pose.position.x, self.current_goal.pose.position.y)\n\n        # Simple path planning (in real implementation, use Isaac\'s planners)\n        path = self.compute_simple_path(start, goal)\n\n        if path:\n            self.current_plan = path\n            self.publish_path(path)\n            self.get_logger().info(f\'New path computed with {len(path)} waypoints\')\n\n    def compute_simple_path(self, start, goal):\n        """Simple path computation (replace with Isaac planners)"""\n        # This is a placeholder - in reality, Isaac would use GPU-accelerated planners\n        # like A*, Dijkstra, or more advanced sampling-based planners\n\n        # Create straight-line path for demonstration\n        steps = 10\n        path = []\n        for i in range(steps + 1):\n            t = i / steps\n            x = start[0] + t * (goal[0] - start[0])\n            y = start[1] + t * (goal[1] - start[1])\n            path.append((x, y))\n\n        return path\n\n    def plan_callback(self):\n        """Periodic planning callback"""\n        self.replan()\n\n    def control_callback(self):\n        """Periodic control callback"""\n        if self.current_plan is None or len(self.current_plan) == 0:\n            # Stop if no plan\n            cmd_vel = Twist()\n            self.cmd_vel_pub.publish(cmd_vel)\n            return\n\n        # Get next waypoint in plan\n        next_waypoint = self.current_plan[0]\n\n        # Calculate control command to reach next waypoint\n        cmd_vel = self.compute_control_to_waypoint(next_waypoint)\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Check if we\'ve reached the waypoint\n        if self.distance_to_waypoint(next_waypoint) < self.get_parameter(\'goal_tolerance\').value:\n            # Remove reached waypoint\n            self.current_plan.pop(0)\n\n            # If no more waypoints, we\'ve reached the goal\n            if len(self.current_plan) == 0:\n                self.get_logger().info(\'Goal reached!\')\n                self.current_goal = None\n                # Stop the robot\n                stop_cmd = Twist()\n                self.cmd_vel_pub.publish(stop_cmd)\n\n    def compute_control_to_waypoint(self, waypoint):\n        """Compute control command to reach a waypoint"""\n        cmd_vel = Twist()\n\n        if self.current_pose is None:\n            return cmd_vel\n\n        # Calculate desired direction\n        dx = waypoint[0] - self.current_pose.position.x\n        dy = waypoint[1] - self.current_pose.position.y\n        distance = np.sqrt(dx*dx + dy*dy)\n\n        # Calculate desired angle\n        desired_yaw = np.arctan2(dy, dx)\n        current_yaw = self.quaternion_to_yaw(self.current_pose.orientation)\n\n        # Simple proportional controller\n        angular_error = self.normalize_angle(desired_yaw - current_yaw)\n\n        # Set velocities\n        max_linear = self.get_parameter(\'max_linear_speed\').value\n        cmd_vel.linear.x = min(max_linear * 0.8, max_linear * distance) if distance > 0.1 else 0.0\n        cmd_vel.angular.z = angular_error * 1.0  # Proportional gain\n\n        # Limit angular velocity\n        max_angular = self.get_parameter(\'max_angular_speed\').value\n        cmd_vel.angular.z = max(-max_angular, min(max_angular, cmd_vel.angular.z))\n\n        return cmd_vel\n\n    def distance_to_waypoint(self, waypoint):\n        """Calculate distance to a waypoint"""\n        if self.current_pose is None:\n            return float(\'inf\')\n\n        dx = waypoint[0] - self.current_pose.position.x\n        dy = waypoint[1] - self.current_pose.position.y\n        return np.sqrt(dx*dx + dy*dy)\n\n    def quaternion_to_yaw(self, orientation):\n        """Convert quaternion to yaw angle"""\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return np.arctan2(siny_cosp, cosy_cosp)\n\n    def normalize_angle(self, angle):\n        """Normalize angle to [-pi, pi] range"""\n        while angle > np.pi:\n            angle -= 2.0 * np.pi\n        while angle < -np.pi:\n            angle += 2.0 * np.pi\n        return angle\n\n    def publish_path(self, path):\n        """Publish the computed path"""\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = \'map\'  # Assuming map frame\n\n        for x, y in path:\n            pose = PoseStamped()\n            pose.header.stamp = path_msg.header.stamp\n            pose.header.frame_id = path_msg.header.frame_id\n            pose.pose.position.x = x\n            pose.pose.position.y = y\n            pose.pose.position.z = 0.0\n            pose.pose.orientation.w = 1.0  # No rotation\n\n            path_msg.poses.append(pose)\n\n        self.path_pub.publish(path_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    navigator = IsaacHumanoidNavigator()\n\n    try:\n        rclpy.spin(navigator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        navigator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-manipulation-example",children:"Isaac Manipulation Example"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-accelerated-grasp-planning",children:"GPU-Accelerated Grasp Planning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Isaac manipulation with GPU-accelerated grasp planning\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom sensor_msgs.msg import PointCloud2\nfrom std_msgs.msg import Bool\nfrom visualization_msgs.msg import Marker\nimport numpy as np\n\nclass IsaacGraspPlanner(Node):\n    def __init__(self):\n        super().__init__(\'isaac_grasp_planner\')\n\n        # Isaac manipulation parameters\n        self.declare_parameter(\'grasp_approach_distance\', 0.1)\n        self.declare_parameter(\'grasp_grasp_distance\', 0.02)\n        self.declare_parameter(\'grasp_elevation_angle\', 0.785)  # 45 degrees\n        self.declare_parameter(\'num_grasp_candidates\', 20)\n\n        # Publishers and subscribers\n        self.object_cloud_sub = self.create_subscription(\n            PointCloud2, \'/object_cloud\',\n            self.object_cloud_callback, 10\n        )\n\n        self.grasp_candidate_pub = self.create_publisher(\n            Marker, \'/grasp_candidates\', 10\n        )\n\n        self.grasp_command_pub = self.create_publisher(\n            Pose, \'/grasp_pose\', 10\n        )\n\n        # Initialize GPU acceleration for grasp planning\n        self.initialize_gpu_grasp_planning()\n\n        self.get_logger().info(\'Isaac Grasp Planner initialized\')\n\n    def initialize_gpu_grasp_planning(self):\n        """Initialize GPU acceleration for grasp planning"""\n        try:\n            import pycuda.driver as cuda\n            import pycuda.autoinit\n            from pycuda.compiler import SourceModule\n\n            # CUDA kernel for grasp evaluation\n            cuda_code = """\n            __global__ void evaluate_grasps_kernel(\n                float* points, int num_points,\n                float* grasp_poses, int num_grasps,\n                float* grasp_scores, int* grasp_validity\n            ) {\n                int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n                if (idx >= num_grasps) return;\n\n                // Get grasp pose\n                float pos_x = grasp_poses[idx * 7 + 0];\n                float pos_y = grasp_poses[idx * 7 + 1];\n                float pos_z = grasp_poses[idx * 7 + 2];\n\n                // Evaluate grasp quality based on point cloud\n                float score = 0.0f;\n                int valid_points = 0;\n\n                for (int i = 0; i < num_points; i++) {\n                    float dx = points[i * 3 + 0] - pos_x;\n                    float dy = points[i * 3 + 1] - pos_y;\n                    float dz = points[i * 3 + 2] - pos_z;\n                    float dist = sqrtf(dx*dx + dy*dy + dz*dz);\n\n                    if (dist < 0.05f) {  // Within grasp distance\n                        score += 1.0f / (dist + 0.001f);  // Higher score for closer points\n                        valid_points++;\n                    }\n                }\n\n                grasp_scores[idx] = score;\n                grasp_validity[idx] = (valid_points >= 5) ? 1 : 0;  // At least 5 points for validity\n            }\n            """\n\n            self.grasp_module = SourceModule(cuda_code)\n            self.evaluate_grasps_kernel = self.grasp_module.get_function("evaluate_grasps_kernel")\n\n            self.gpu_available = True\n            self.get_logger().info(\'GPU-accelerated grasp planning enabled\')\n\n        except ImportError:\n            self.gpu_available = False\n            self.get_logger().warn(\'CUDA not available for grasp planning, using CPU fallback\')\n\n    def object_cloud_callback(self, msg: PointCloud2):\n        """Process object point cloud for grasp planning"""\n        try:\n            import struct\n            # Convert PointCloud2 to numpy array (simplified)\n            # In a real implementation, use sensor_msgs_py.point_cloud2.read_points\n            points = self.pointcloud2_to_array(msg)\n\n            if len(points) == 0:\n                self.get_logger().warn(\'Empty point cloud received\')\n                return\n\n            # Generate grasp candidates\n            grasp_poses = self.generate_grasp_candidates(points)\n\n            # Evaluate grasps using GPU acceleration\n            if self.gpu_available:\n                best_grasp = self.evaluate_grasps_gpu(points, grasp_poses)\n            else:\n                best_grasp = self.evaluate_grasps_cpu(points, grasp_poses)\n\n            if best_grasp is not None:\n                # Publish the best grasp\n                self.grasp_command_pub.publish(best_grasp)\n\n                # Visualize grasp candidates\n                self.visualize_grasp_candidates(grasp_poses, best_grasp)\n\n                self.get_logger().info(\'Published grasp pose for manipulation\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in grasp planning: {e}\')\n\n    def pointcloud2_to_array(self, cloud_msg):\n        """Convert PointCloud2 message to numpy array (simplified)"""\n        # This is a simplified conversion - in practice use sensor_msgs_py.point_cloud2\n        # For this example, return a dummy array\n        return np.random.rand(100, 3).astype(np.float32) * 0.5  # 100 random points in 0.5m cube\n\n    def generate_grasp_candidates(self, points):\n        """Generate candidate grasp poses based on point cloud"""\n        num_candidates = self.get_parameter(\'num_grasp_candidates\').value\n\n        # Find centroid of point cloud\n        centroid = np.mean(points, axis=0)\n\n        # Generate grasps around the centroid\n        grasps = []\n        for i in range(num_candidates):\n            # Random offset from centroid\n            offset = np.random.normal(0, 0.05, 3)  # 5cm random offset\n            position = centroid + offset\n\n            # Random orientation (simplified)\n            roll = np.random.uniform(-np.pi, np.pi)\n            pitch = np.random.uniform(-np.pi/4, np.pi/4)  # Limited pitch for humanoid\n            yaw = np.random.uniform(-np.pi, np.pi)\n\n            # Convert to quaternion\n            cy = np.cos(yaw * 0.5)\n            sy = np.sin(yaw * 0.5)\n            cp = np.cos(pitch * 0.5)\n            sp = np.sin(pitch * 0.5)\n            cr = np.cos(roll * 0.5)\n            sr = np.sin(roll * 0.5)\n\n            w = cr * cp * cy + sr * sp * sy\n            x = sr * cp * cy - cr * sp * sy\n            y = cr * sp * cy + sr * cp * sy\n            z = cr * cp * sy - sr * sp * cy\n\n            grasp_pose = np.array([position[0], position[1], position[2], x, y, z, w], dtype=np.float32)\n            grasps.append(grasp_pose)\n\n        return np.array(grasps, dtype=np.float32)\n\n    def evaluate_grasps_gpu(self, points, grasp_poses):\n        """GPU-accelerated grasp evaluation"""\n        num_points = len(points)\n        num_grasps = len(grasp_poses)\n\n        # Flatten grasp poses\n        grasp_poses_flat = grasp_poses.flatten()\n\n        # Allocate GPU memory\n        points_gpu = cuda.mem_alloc(points.nbytes)\n        grasps_gpu = cuda.mem_alloc(grasp_poses_flat.nbytes)\n        scores_gpu = cuda.mem_alloc(num_grasps * 4)  # float32\n        validity_gpu = cuda.mem_alloc(num_grasps * 4)  # int32\n\n        # Copy data to GPU\n        cuda.memcpy_htod(points_gpu, points.astype(np.float32))\n        cuda.memcpy_htod(grasps_gpu, grasp_poses_flat)\n\n        # Configure kernel execution\n        block_size = 256\n        grid_size = (num_grasps + block_size - 1) // block_size\n\n        # Execute kernel\n        self.evaluate_grasps_kernel(\n            points_gpu, np.int32(num_points),\n            grasps_gpu, np.int32(num_grasps),\n            scores_gpu, validity_gpu,\n            block=(block_size, 1, 1),\n            grid=(grid_size, 1)\n        )\n\n        # Copy results back\n        scores = np.empty(num_grasps, dtype=np.float32)\n        validity = np.empty(num_grasps, dtype=np.int32)\n        cuda.memcpy_dtoh(scores, scores_gpu)\n        cuda.memcpy_dtoh(validity, validity_gpu)\n\n        # Clean up GPU memory\n        del points_gpu\n        del grasps_gpu\n        del scores_gpu\n        del validity_gpu\n\n        # Find best valid grasp\n        valid_scores = scores * validity  # Zero out invalid grasps\n        if np.any(validity > 0):\n            best_idx = np.argmax(valid_scores)\n            best_grasp = self.array_to_pose(grasp_poses[best_idx])\n            return best_grasp\n\n        return None\n\n    def evaluate_grasps_cpu(self, points, grasp_poses):\n        """CPU-based grasp evaluation as fallback"""\n        best_score = -1\n        best_grasp = None\n\n        for i, grasp in enumerate(grasp_poses):\n            score = 0\n            valid_points = 0\n\n            pos = grasp[:3]\n\n            for point in points:\n                dist = np.linalg.norm(point - pos)\n                if dist < 0.05:  # Within grasp distance\n                    score += 1.0 / (dist + 0.001)  # Higher score for closer points\n                    valid_points += 1\n\n            if valid_points >= 5:  # At least 5 points for validity\n                if score > best_score:\n                    best_score = score\n                    best_grasp = self.array_to_pose(grasp)\n\n        return best_grasp\n\n    def array_to_pose(self, grasp_array):\n        """Convert grasp array to Pose message"""\n        from geometry_msgs.msg import Pose\n        pose = Pose()\n        pose.position.x = grasp_array[0]\n        pose.position.y = grasp_array[1]\n        pose.position.z = grasp_array[2]\n        pose.orientation.x = grasp_array[3]\n        pose.orientation.y = grasp_array[4]\n        pose.orientation.z = grasp_array[5]\n        pose.orientation.w = grasp_array[6]\n        return pose\n\n    def visualize_grasp_candidates(self, grasp_poses, best_grasp):\n        """Visualize grasp candidates in RViz"""\n        # This would create visualization markers for RViz\n        # For simplicity, just log the best grasp\n        self.get_logger().info(f\'Best grasp: ({best_grasp.position.x:.3f}, {best_grasp.position.y:.3f}, {best_grasp.position.z:.3f})\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    grasp_planner = IsaacGraspPlanner()\n\n    try:\n        rclpy.spin(grasp_planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        grasp_planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-integration-examples",children:"Isaac Integration Examples"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-sim-to-real-robot-transfer",children:"Isaac Sim to Real Robot Transfer"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example: Isaac Sim-to-Real transfer pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32MultiArray\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import JointTrajectoryControllerState\nimport numpy as np\n\nclass IsaacSimToRealTransfer(Node):\n    def __init__(self):\n        super().__init__('isaac_sim_to_real_transfer')\n\n        # Isaac sim-to-real transfer parameters\n        self.declare_parameter('domain_randomization_enabled', True)\n        self.declare_parameter('parameter_variance', 0.1)\n        self.declare_parameter('transfer_validation_threshold', 0.95)\n\n        # Publishers and subscribers for sim and real robots\n        self.sim_joint_pub = self.create_publisher(JointState, '/sim/joint_commands', 10)\n        self.real_joint_pub = self.create_publisher(JointState, '/real/joint_commands', 10)\n\n        self.sim_feedback_sub = self.create_subscription(\n            JointState, '/sim/joint_states',\n            self.sim_feedback_callback, 10\n        )\n\n        self.real_feedback_sub = self.create_subscription(\n            JointState, '/real/joint_states',\n            self.real_feedback_callback, 10\n        )\n\n        # Transfer validation publisher\n        self.validation_pub = self.create_publisher(Float32MultiArray, '/transfer_validation', 10)\n\n        # State tracking\n        self.sim_states = {}\n        self.real_states = {}\n        self.sim_to_real_mapping = {}  # Maps sim joint names to real joint names\n\n        self.get_logger().info('Isaac Sim-to-Real Transfer initialized')\n\n    def sim_feedback_callback(self, msg: JointState):\n        \"\"\"Process simulation robot feedback\"\"\"\n        self.sim_states = dict(zip(msg.name, msg.position))\n\n        # Apply domain randomization to simulation parameters\n        if self.get_parameter('domain_randomization_enabled').value:\n            randomized_states = self.apply_domain_randomization(msg)\n        else:\n            randomized_states = msg\n\n        # Transfer to real robot with appropriate mapping and corrections\n        self.transfer_to_real_robot(randomized_states)\n\n    def real_feedback_callback(self, msg: JointState):\n        \"\"\"Process real robot feedback\"\"\"\n        self.real_states = dict(zip(msg.name, msg.position))\n\n        # Validate sim-to-real transfer performance\n        self.validate_transfer_performance()\n\n    def apply_domain_randomization(self, joint_state_msg):\n        \"\"\"Apply domain randomization to simulation parameters\"\"\"\n        # Randomize joint positions slightly\n        variance = self.get_parameter('parameter_variance').value\n        randomized_positions = []\n\n        for pos in joint_state_msg.position:\n            random_offset = np.random.normal(0, variance)\n            randomized_positions.append(pos + random_offset)\n\n        # Create new message with randomized values\n        randomized_msg = JointState()\n        randomized_msg.header = joint_state_msg.header\n        randomized_msg.name = joint_state_msg.name\n        randomized_msg.position = randomized_positions\n        randomized_msg.velocity = [v + np.random.normal(0, variance*0.1) for v in joint_state_msg.velocity]\n        randomized_msg.effort = [e + np.random.normal(0, variance*0.5) for e in joint_state_msg.effort]\n\n        return randomized_msg\n\n    def transfer_to_real_robot(self, sim_command):\n        \"\"\"Transfer simulation command to real robot with corrections\"\"\"\n        real_command = JointState()\n        real_command.header.stamp = self.get_clock().now().to_msg()\n        real_command.header.frame_id = 'base_link'\n\n        # Map simulation joints to real robot joints\n        for sim_name, sim_pos in zip(sim_command.name, sim_command.position):\n            if sim_name in self.sim_to_real_mapping:\n                real_name = self.sim_to_real_mapping[sim_name]\n                real_command.name.append(real_name)\n                # Apply any sim-to-real corrections here\n                corrected_pos = self.apply_corrections(sim_name, sim_pos)\n                real_command.position.append(corrected_pos)\n\n        # Publish command to real robot\n        self.real_joint_pub.publish(real_command)\n\n    def apply_corrections(self, joint_name, sim_position):\n        \"\"\"Apply sim-to-real corrections to joint positions\"\"\"\n        # This would contain learned correction functions from sim-to-real transfer\n        # For this example, apply simple corrections based on known differences\n\n        # Example: Known offset for a specific joint\n        if joint_name == 'left_hip_pitch':\n            # Apply learned correction factor\n            correction_factor = 0.98  # Learned from system identification\n            offset = 0.02  # Fixed offset\n            return sim_position * correction_factor + offset\n        elif joint_name == 'right_knee_pitch':\n            # Another learned correction\n            correction_factor = 1.02\n            offset = -0.01\n            return sim_position * correction_factor + offset\n        else:\n            # Default correction\n            return sim_position\n\n    def validate_transfer_performance(self):\n        \"\"\"Validate sim-to-real transfer performance\"\"\"\n        if not self.sim_states or not self.real_states:\n            return\n\n        # Calculate similarity between sim and real robot states\n        similarities = []\n\n        for joint_name in self.sim_states:\n            if joint_name in self.real_states:\n                sim_pos = self.sim_states[joint_name]\n                real_pos = self.real_states[joint_name]\n\n                # Calculate similarity (1.0 = identical, 0.0 = completely different)\n                # Using a simple difference measure (in practice, use more sophisticated metrics)\n                diff = abs(sim_pos - real_pos)\n                similarity = max(0.0, 1.0 - diff)  # Simple linear similarity\n                similarities.append(similarity)\n\n        if similarities:\n            avg_similarity = sum(similarities) / len(similarities)\n\n            # Publish validation result\n            validation_msg = Float32MultiArray()\n            validation_msg.data = [avg_similarity]\n            self.validation_pub.publish(validation_msg)\n\n            self.get_logger().info(f'Transfer validation: {avg_similarity:.3f}')\n\n            # Check if transfer is performing adequately\n            threshold = self.get_parameter('transfer_validation_threshold').value\n            if avg_similarity < threshold:\n                self.get_logger().warn(f'Transfer performance below threshold: {avg_similarity:.3f} < {threshold:.3f}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    transfer_node = IsaacSimToRealTransfer()\n\n    try:\n        rclpy.spin(transfer_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        transfer_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ai-integration-example",children:"Isaac AI Integration Example"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-accelerated-perception-with-isaac",children:"GPU-Accelerated Perception with Isaac"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example: Isaac AI integration for perception\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point\nimport numpy as np\n\nclass IsaacAIPerception(Node):\n    def __init__(self):\n        super().__init__('isaac_ai_perception')\n\n        # Isaac AI parameters\n        self.declare_parameter('confidence_threshold', 0.5)\n        self.declare_parameter('nms_threshold', 0.4)\n        self.declare_parameter('max_objects', 10)\n\n        # Publishers and subscribers\n        self.rgb_sub = self.create_subscription(\n            Image, '/camera/rgb/image_rect_color',\n            self.rgb_callback, 10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth/image_rect',\n            self.depth_callback, 10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/rgb/camera_info',\n            self.camera_info_callback, 10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/ai/detections', 10\n        )\n\n        # Initialize Isaac AI components\n        self.initialize_isaac_ai()\n\n        # Camera parameters\n        self.camera_intrinsics = None\n\n        self.get_logger().info('Isaac AI Perception initialized')\n\n    def initialize_isaac_ai(self):\n        \"\"\"Initialize Isaac AI components\"\"\"\n        try:\n            import torch\n            import torchvision.transforms as transforms\n\n            # Initialize GPU acceleration if available\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n            # Load pre-trained model (in practice, load Isaac-specific models)\n            # For this example, we'll use a placeholder\n            self.model = None  # Would load actual model\n            self.transforms = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Resize((416, 416))\n            ])\n\n            self.ai_initialized = True\n            self.get_logger().info(f'Isaac AI initialized on {self.device}')\n\n        except ImportError:\n            self.ai_initialized = False\n            self.device = torch.device('cpu')\n            self.get_logger().warn('PyTorch not available, using CPU for AI processing')\n\n    def rgb_callback(self, msg: Image):\n        \"\"\"Process RGB image for AI-based perception\"\"\"\n        if not self.ai_initialized or self.camera_intrinsics is None:\n            return\n\n        try:\n            # Convert ROS image to OpenCV\n            from cv_bridge import CvBridge\n            cv_bridge = CvBridge()\n            cv_image = cv_bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n            # Run AI inference\n            if self.model is not None:\n                detections = self.run_inference(cv_image)\n            else:\n                # Placeholder detections\n                detections = self.placeholder_detection(cv_image)\n\n            # Create and publish detection message\n            detection_msg = self.create_detection_message(detections, msg.header)\n            self.detection_pub.publish(detection_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in AI perception: {e}')\n\n    def depth_callback(self, msg: Image):\n        \"\"\"Process depth image to add 3D information to detections\"\"\"\n        # In a complete implementation, this would be used to convert 2D detections to 3D positions\n        pass\n\n    def camera_info_callback(self, msg: CameraInfo):\n        \"\"\"Store camera intrinsics for 3D reconstruction\"\"\"\n        self.camera_intrinsics = np.array(msg.k).reshape(3, 3)\n\n    def run_inference(self, image):\n        \"\"\"Run AI inference on image\"\"\"\n        # Preprocess image\n        input_tensor = self.transforms(image).unsqueeze(0).to(self.device)\n\n        # Run inference (placeholder)\n        # In a real implementation, this would use Isaac's optimized inference\n        with torch.no_grad():\n            # Placeholder: return dummy detections\n            # Real implementation would call self.model(input_tensor)\n            return self.placeholder_detection(image)\n\n    def placeholder_detection(self, image):\n        \"\"\"Placeholder detection function\"\"\"\n        # This would be replaced with actual AI model inference\n        # For demonstration, return random detections\n        height, width = image.shape[:2]\n\n        num_detections = np.random.randint(0, 4)  # 0-3 random detections\n        detections = []\n\n        for i in range(num_detections):\n            # Random bounding box\n            x = np.random.randint(0, width - 100)\n            y = np.random.randint(0, height - 100)\n            w = np.random.randint(50, 150)\n            h = np.random.randint(50, 150)\n\n            # Random confidence\n            confidence = np.random.uniform(0.5, 0.99)\n\n            # Random class\n            classes = ['person', 'chair', 'table', 'cup']\n            class_name = np.random.choice(classes)\n\n            detection = {\n                'bbox': (x, y, w, h),\n                'confidence': confidence,\n                'class': class_name\n            }\n            detections.append(detection)\n\n        return detections\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create vision_msgs/Detection2DArray message from detections\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for det in detections:\n            detection_msg = Detection2D()\n\n            # Set bounding box\n            bbox = det['bbox']\n            detection_msg.bbox.center.x = bbox[0] + bbox[2] / 2  # center x\n            detection_msg.bbox.center.y = bbox[1] + bbox[3] / 2  # center y\n            detection_msg.bbox.size_x = bbox[2]  # width\n            detection_msg.bbox.size_y = bbox[3]  # height\n\n            # Set hypothesis\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = det['class']\n            hypothesis.score = det['confidence']\n            detection_msg.results.append(hypothesis)\n\n            detection_array.detections.append(detection_msg)\n\n        return detection_array\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    ai_perception = IsaacAIPerception()\n\n    try:\n        rclpy.spin(ai_perception)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_perception.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-launch-files",children:"Isaac Launch Files"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-navigation-launch-example",children:"Isaac Navigation Launch Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="isaac_navigation.launch.py"',children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    declare_use_sim_time_arg = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation clock if true'\n    )\n\n    enable_viz = LaunchConfiguration('enable_viz')\n    declare_enable_viz_arg = DeclareLaunchArgument(\n        'enable_viz',\n        default_value='true',\n        description='Enable visualization'\n    )\n\n    # Isaac Navigation nodes\n    navigation_node = Node(\n        package='isaac_navigation',\n        executable='isaac_nav_node',\n        name='isaac_humanoid_navigator',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('isaac_navigation'),\n                'config',\n                'nav_params.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        remappings=[\n            ('/cmd_vel', '/humanoid/cmd_vel'),\n            ('/scan', '/humanoid/laser_scan'),\n            ('/map', '/humanoid/map'),\n            ('/move_base_simple/goal', '/humanoid/goal'),\n        ],\n        output='screen'\n    )\n\n    # Isaac Perception node\n    perception_node = Node(\n        package='isaac_perception',\n        executable='isaac_perception_node',\n        name='isaac_obstacle_detector',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'enable_gpu_processing': True}\n        ],\n        remappings=[\n            ('/input_image', '/humanoid/front_camera/image_rect_color'),\n            ('/obstacles', '/humanoid/obstacles'),\n        ],\n        output='screen'\n    )\n\n    # Isaac SLAM node (if needed)\n    slam_node = Node(\n        package='isaac_slam',\n        executable='isaac_slam_node',\n        name='isaac_vslam',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'enable_gpu_slam': True}\n        ],\n        remappings=[\n            ('/camera/color/image_raw', '/humanoid/stereo_camera/left/image_rect_color'),\n            ('/camera/depth/image_rect', '/humanoid/stereo_camera/depth/image_rect'),\n            ('/map', '/humanoid/vslam_map'),\n            ('/tf', '/tf'),\n            ('/tf_static', '/tf_static'),\n        ],\n        output='screen'\n    )\n\n    # RViz2 (if visualization enabled)\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', PathJoinSubstitution([\n            FindPackageShare('isaac_navigation'),\n            'rviz',\n            'navigation.rviz'\n        ])],\n        parameters=[{'use_sim_time': use_sim_time}],\n        condition=IfCondition(enable_viz)\n    )\n\n    return LaunchDescription([\n        declare_use_sim_time_arg,\n        declare_enable_viz_arg,\n        navigation_node,\n        perception_node,\n        slam_node,\n        rviz_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-configuration-files",children:"Isaac Configuration Files"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-parameter-configuration",children:"Isaac Parameter Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:'title="config/nav_params.yaml"',children:'isaac_humanoid_navigator:\n  ros__parameters:\n    # Controller parameters\n    controller_frequency: 50.0\n    min_x_velocity_threshold: 0.001\n    max_x_velocity: 0.5\n    min_y_velocity_threshold: 0.001\n    max_y_velocity: 0.2\n    max_theta_velocity: 1.0\n    min_theta_velocity_threshold: 0.001\n\n    # Goal checker parameters\n    goal_checker.xy_goal_tolerance: 0.25\n    goal_checker.yaw_goal_tolerance: 0.05\n    goal_checker.stateful: True\n\n    # Local planner (Trajectory follower)\n    local_costmap:\n      global_frame: odom\n      robot_base_frame: base_link\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      footprint: "[[-0.325, -0.325], [-0.325, 0.325], [0.325, 0.325], [0.325, -0.325]]"\n      resolution: 0.05\n      inflation_radius: 0.55\n      plugins: ["obstacle_layer", "inflation_layer"]\n\n    local_costmap/obstacle_layer:\n      plugin: "nav2_costmap_2d::ObstacleLayer"\n      enabled: True\n      observation_sources: scan\n      scan:\n        topic: /scan\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "LaserScan"\n        raytrace_max_range: 3.0\n        raytrace_min_range: 0.0\n        obstacle_max_range: 2.5\n        obstacle_min_range: 0.0\n\n    local_costmap/inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      enabled: True\n      inflation_radius: 0.55\n      cost_scaling_factor: 3.0\n      inflate_unknown: False\n      inflate_around_unknown: False\n'})})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var s=a(6540);const i={},t=s.createContext(i);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);