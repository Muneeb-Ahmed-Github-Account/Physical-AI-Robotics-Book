"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[2100],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}},9254:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"capstone-humanoid/pipeline-example","title":"Complete SPPA Pipeline Example","description":"Complete simulate \u2192 perceive \u2192 plan \u2192 act pipeline example integrating all capstone project concepts","source":"@site/docs/capstone-humanoid/pipeline-example.md","sourceDirName":"capstone-humanoid","slug":"/capstone-humanoid/pipeline-example","permalink":"/Physical-AI-Robotics-Book/docs/capstone-humanoid/pipeline-example","draft":false,"unlisted":false,"editUrl":"https://github.com/Muneeb-Ahmed-Github-Account/Physical-AI-Robotics-Book/tree/main/docs/capstone-humanoid/pipeline-example.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Complete SPPA Pipeline Example","sidebar_position":6,"description":"Complete simulate \u2192 perceive \u2192 plan \u2192 act pipeline example integrating all capstone project concepts"}}');var s=t(4848),o=t(8453);const a={title:"Complete SPPA Pipeline Example",sidebar_position:6,description:"Complete simulate \u2192 perceive \u2192 plan \u2192 act pipeline example integrating all capstone project concepts"},r="Complete Simulate \u2192 Perceive \u2192 Plan \u2192 Act Pipeline Example",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Complete Pipeline Architecture",id:"complete-pipeline-architecture",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Complete Implementation Example",id:"complete-implementation-example",level:2},{value:"1. Simulation Environment",id:"1-simulation-environment",level:3},{value:"2. Perception System",id:"2-perception-system",level:3},{value:"3. Planning System",id:"3-planning-system",level:3},{value:"4. Action System",id:"4-action-system",level:3},{value:"5. Safety System Integration",id:"5-safety-system-integration",level:3},{value:"Complete Integration Example",id:"complete-integration-example",level:2},{value:"Main Pipeline Node",id:"main-pipeline-node",level:3},{value:"Real-World Deployment Example",id:"real-world-deployment-example",level:2},{value:"Deployment Configuration",id:"deployment-configuration",level:3},{value:"Testing the Complete Pipeline",id:"testing-the-complete-pipeline",level:2},{value:"Integration Test",id:"integration-test",level:3},{value:"Cross-References",id:"cross-references",level:2},{value:"References",id:"references",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"complete-simulate--perceive--plan--act-pipeline-example",children:"Complete Simulate \u2192 Perceive \u2192 Plan \u2192 Act Pipeline Example"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After studying this complete pipeline example, students will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the complete flow from simulation to physical action [1]"}),"\n",(0,s.jsx)(n.li,{children:"Implement integrated perception, planning, and action systems [2]"}),"\n",(0,s.jsx)(n.li,{children:"Connect all components of the humanoid robotics pipeline [3]"}),"\n",(0,s.jsx)(n.li,{children:"Handle real-time processing requirements across all stages [4]"}),"\n",(0,s.jsx)(n.li,{children:"Integrate safety mechanisms throughout the pipeline [5]"}),"\n",(0,s.jsx)(n.li,{children:"Optimize performance for end-to-end operation [6]"}),"\n",(0,s.jsx)(n.li,{children:"Debug complex pipeline interactions [7]"}),"\n",(0,s.jsx)(n.li,{children:"Validate complete pipeline functionality [8]"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate pipeline performance metrics [9]"}),"\n",(0,s.jsx)(n.li,{children:"Document and present pipeline implementations [10]"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"complete-pipeline-architecture",children:"Complete Pipeline Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,s.jsx)(n.p,{children:"The complete Simulate \u2192 Perceive \u2192 Plan \u2192 Act (SPPA) pipeline integrates all components into a cohesive system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Simulation    \u2502\u2500\u2500\u2500\u25b6\u2502   Perception     \u2502\u2500\u2500\u2500\u25b6\u2502    Planning      \u2502\u2500\u2500\u2500\u25b6\u2502     Action       \u2502\u2500\u2500\u2500\u25b6\u2502   Physical       \u2502\n\u2502   Environment   \u2502    \u2502   System         \u2502    \u2502   System         \u2502    \u2502   System         \u2502    \u2502   Execution     \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502                  \u2502    \u2502                  \u2502    \u2502                 \u2502\n\u2502 \u2022 Physics       \u2502    \u2502 \u2022 Visual         \u2502    \u2502 \u2022 Task           \u2502    \u2502 \u2022 Motion         \u2502    \u2502 \u2022 Navigation    \u2502\n\u2502 \u2022 Sensors       \u2502    \u2502   Processing     \u2502    \u2502   Planning       \u2502    \u2502   Execution      \u2502    \u2502 \u2022 Manipulation  \u2502\n\u2502 \u2022 Environment   \u2502    \u2502 \u2022 Language       \u2502    \u2502 \u2022 Motion         \u2502    \u2502 \u2022 Control        \u2502    \u2502 \u2022 Interaction   \u2502\n\u2502 \u2022 Objects       \u2502    \u2502   Understanding  \u2502    \u2502   Planning       \u2502    \u2502 \u2022 Feedback       \u2502    \u2502 \u2022 Results       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"complete-implementation-example",children:"Complete Implementation Example"}),"\n",(0,s.jsx)(n.h3,{id:"1-simulation-environment",children:"1. Simulation Environment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Complete simulation environment with realistic physics\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu, JointState\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import String, Bool\nimport numpy as np\nimport cv2\nimport torch\nfrom transforms3d.euler import euler2quat, quat2euler\nimport time\nimport threading\nfrom queue import Queue\n\nclass CompleteSimulationEnvironment(Node):\n    def __init__(self):\n        super().__init__('complete_simulation_env')\n\n        # Publishers for all sensor modalities\n        self.camera_pub = self.create_publisher(Image, '/camera/rgb/image_raw', 10)\n        self.depth_pub = self.create_publisher(Image, '/camera/depth/image_raw', 10)\n        self.laser_pub = self.create_publisher(LaserScan, '/scan', 10)\n        self.imu_pub = self.create_publisher(Imu, '/imu', 10)\n        self.joint_state_pub = self.create_publisher(JointState, '/joint_states', 10)\n\n        # Subscribers for robot commands\n        self.cmd_vel_sub = self.create_subscription(Twist, '/cmd_vel', self.cmd_vel_callback, 10)\n        self.joint_cmd_sub = self.create_subscription(JointState, '/joint_commands', self.joint_cmd_callback, 10)\n\n        # Setup simulation environment\n        self.setup_environment()\n\n        # Start simulation loop\n        self.simulation_rate = 100  # Hz\n        self.timer = self.create_timer(1.0/self.simulation_rate, self.simulation_step)\n\n        # Robot state tracking\n        self.robot_state = {\n            'position': np.array([0.0, 0.0, 0.0]),  # x, y, theta\n            'velocity': np.array([0.0, 0.0, 0.0]),\n            'joints': np.zeros(32),  # Assuming 32 DoF humanoid\n            'command': Twist()\n        }\n\n        self.get_logger().info(\"Complete simulation environment initialized\")\n\n    def setup_environment(self):\n        \"\"\"Setup complete simulation environment with objects and scenarios.\"\"\"\n        self.environment = {\n            'objects': [\n                {'name': 'table', 'type': 'furniture', 'position': [2.0, 1.0, 0.0], 'size': [1.0, 0.5, 0.8]},\n                {'name': 'ball', 'type': 'movable', 'position': [2.2, 1.2, 0.8], 'size': [0.1, 0.1, 0.1]},\n                {'name': 'chair', 'type': 'furniture', 'position': [3.0, 0.0, 0.0], 'size': [0.5, 0.5, 0.8]},\n                {'name': 'human', 'type': 'dynamic', 'position': [1.0, 0.5, 0.0], 'velocity': [0.1, 0.0, 0.0]}\n            ],\n            'obstacles': [\n                {'position': [1.5, 0.5, 0.0], 'radius': 0.3}\n            ],\n            'navigation_goals': [\n                {'name': 'kitchen', 'position': [3.0, 2.0, 0.0]},\n                {'name': 'living_room', 'position': [0.0, 2.0, 0.0]}\n            ]\n        }\n\n        # Setup physics engine parameters\n        self.physics_params = {\n            'gravity': [0, 0, -9.81],\n            'friction': 0.5,\n            'restitution': 0.1\n        }\n\n    def simulation_step(self):\n        \"\"\"Main simulation update loop.\"\"\"\n        # Update physics\n        self.update_physics()\n\n        # Update sensor readings\n        self.update_sensors()\n\n        # Publish all sensor data\n        self.publish_sensor_data()\n\n        # Update environment dynamics (moving objects, humans, etc.)\n        self.update_dynamic_objects()\n\n    def update_physics(self):\n        \"\"\"Update physics simulation.\"\"\"\n        # Apply robot motion based on current command\n        cmd = self.robot_state['command']\n\n        # Simple differential drive model for base\n        linear_vel = np.sqrt(cmd.linear.x**2 + cmd.linear.y**2)\n        angular_vel = cmd.angular.z\n\n        # Update position using kinematic model\n        dt = 1.0 / self.simulation_rate\n        self.robot_state['position'][0] += linear_vel * np.cos(self.robot_state['position'][2]) * dt\n        self.robot_state['position'][1] += linear_vel * np.sin(self.robot_state['position'][2]) * dt\n        self.robot_state['position'][2] += angular_vel * dt\n\n        # Update joint positions based on commands\n        # This is simplified - real implementation would use joint dynamics\n        for i in range(len(self.robot_state['joints'])):\n            if i < len(self.robot_state['joints']):  # If we have a command for this joint\n                # Apply joint movement with velocity limits\n                pass\n\n    def update_sensors(self):\n        \"\"\"Update all sensor readings.\"\"\"\n        # Generate camera image\n        camera_image = self.generate_camera_image()\n        self.publish_camera_image(camera_image)\n\n        # Generate depth image\n        depth_image = self.generate_depth_image()\n        self.publish_depth_image(depth_image)\n\n        # Generate laser scan\n        laser_scan = self.generate_laser_scan()\n        self.publish_laser_scan(laser_scan)\n\n        # Generate IMU data\n        imu_data = self.generate_imu_data()\n        self.publish_imu_data(imu_data)\n\n        # Generate joint states\n        joint_states = self.generate_joint_states()\n        self.publish_joint_states(joint_states)\n\n    def generate_camera_image(self):\n        \"\"\"Generate realistic camera image from robot perspective.\"\"\"\n        # Create image based on robot position and environment\n        image = np.zeros((480, 640, 3), dtype=np.uint8)\n\n        # Draw environment objects in image\n        robot_pos = self.robot_state['position']\n        for obj in self.environment['objects']:\n            obj_pos = obj['position']\n            # Calculate relative position and draw object\n            rel_x = obj_pos[0] - robot_pos[0]\n            rel_y = obj_pos[1] - robot_pos[1]\n\n            # Simple projection to image coordinates\n            # In real implementation, use proper camera model\n            img_x = int(320 + rel_x * 100)  # Scale factor for visualization\n            img_y = int(240 - rel_y * 100)  # Flip Y axis\n\n            if 0 <= img_x < 640 and 0 <= img_y < 480:\n                color = (255, 0, 0) if obj['type'] == 'furniture' else (0, 255, 0)\n                cv2.circle(image, (img_x, img_y), 10, color, -1)\n\n        return image\n\n    def generate_laser_scan(self):\n        \"\"\"Generate realistic laser scan data.\"\"\"\n        # Create scan with 360 points\n        scan_ranges = np.full(360, 10.0)  # Default max range\n\n        robot_pos = self.robot_state['position']\n\n        # Calculate distances to obstacles\n        for i in range(360):\n            angle = np.radians(i) + robot_pos[2]  # Add robot orientation\n\n            # Check distance to each obstacle\n            for obj in self.environment['objects']:\n                obj_pos = np.array(obj['position'][:2])\n                robot_2d_pos = np.array(robot_pos[:2])\n\n                # Vector from robot to object\n                to_obj = obj_pos - robot_2d_pos\n                obj_distance = np.linalg.norm(to_obj)\n\n                # Angle from robot to object\n                obj_angle = np.arctan2(to_obj[1], to_obj[0])\n\n                # Check if object is in this scan direction (with some tolerance)\n                angle_diff = np.abs(angle - obj_angle)\n                if angle_diff < np.radians(2) or angle_diff > np.radians(358):  # ~2 degree tolerance\n                    if obj_distance < scan_ranges[i]:\n                        scan_ranges[i] = obj_distance\n\n        return scan_ranges\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Handle velocity commands from the pipeline.\"\"\"\n        self.robot_state['command'] = msg\n\n    def joint_cmd_callback(self, msg):\n        \"\"\"Handle joint commands from the pipeline.\"\"\"\n        # Update joint commands\n        for i, name in enumerate(msg.name):\n            if name in self.joint_indices:\n                idx = self.joint_indices[name]\n                if i < len(msg.position):\n                    self.joint_targets[idx] = msg.position[i]\n\n    def update_dynamic_objects(self):\n        \"\"\"Update positions of dynamic objects (humans, moving objects).\"\"\"\n        for obj in self.environment['objects']:\n            if obj['type'] == 'dynamic':\n                # Move object based on its velocity\n                obj['position'][0] += obj['velocity'][0] / self.simulation_rate\n                obj['position'][1] += obj['velocity'][1] / self.simulation_rate\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-perception-system",children:"2. Perception System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Complete perception system for multimodal processing\nclass CompletePerceptionSystem(Node):\n    def __init__(self):\n        super().__init__('complete_perception_system')\n\n        # Subscribers for all sensor modalities\n        self.camera_sub = self.create_subscription(Image, '/camera/rgb/image_raw', self.camera_callback, 10)\n        self.depth_sub = self.create_subscription(Image, '/camera/depth/image_raw', self.depth_callback, 10)\n        self.laser_sub = self.create_subscription(LaserScan, '/scan', self.laser_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, '/imu', self.imu_callback, 10)\n\n        # Publisher for perception results\n        self.perception_pub = self.create_publisher(String, '/perception/results', 10)\n\n        # Publisher for detected objects\n        self.objects_pub = self.create_publisher(String, '/perception/objects', 10)\n\n        # Initialize perception models\n        self.initialize_perception_models()\n\n        # Perception state\n        self.latest_data = {\n            'image': None,\n            'depth': None,\n            'laser': None,\n            'imu': None\n        }\n\n        self.get_logger().info(\"Complete perception system initialized\")\n\n    def initialize_perception_models(self):\n        \"\"\"Initialize all perception models.\"\"\"\n        # Initialize computer vision models\n        self.object_detector = self.initialize_object_detector()\n        self.segmentation_model = self.initialize_segmentation_model()\n        self.pose_estimator = self.initialize_pose_estimator()\n\n        # Initialize language processing models\n        self.language_processor = self.initialize_language_processor()\n\n        # Initialize sensor fusion system\n        self.fusion_system = SensorFusionSystem()\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera image.\"\"\"\n        # Convert ROS image to OpenCV format\n        cv_image = self.ros_image_to_cv2(msg)\n\n        # Store latest image\n        self.latest_data['image'] = cv_image\n\n        # Process image if we have other sensor data\n        if self.all_sensors_ready():\n            self.process_multimodal_data()\n\n    def laser_callback(self, msg):\n        \"\"\"Process laser scan data.\"\"\"\n        # Convert laser scan to numpy array\n        laser_ranges = np.array(msg.ranges)\n\n        # Store latest laser data\n        self.latest_data['laser'] = laser_ranges\n\n        # Process if all data available\n        if self.all_sensors_ready():\n            self.process_multimodal_data()\n\n    def all_sensors_ready(self):\n        \"\"\"Check if all sensor data is available.\"\"\"\n        return all(data is not None for data in self.latest_data.values())\n\n    def process_multimodal_data(self):\n        \"\"\"Process all sensor data in a fused manner.\"\"\"\n        # Extract visual information\n        visual_features = self.process_visual_data(self.latest_data['image'])\n\n        # Extract spatial information from laser\n        spatial_features = self.process_spatial_data(self.latest_data['laser'])\n\n        # Extract motion information from IMU\n        motion_features = self.process_motion_data(self.latest_data['imu'])\n\n        # Fuse all features\n        fused_features = self.fusion_system.fuse_features(\n            visual_features, spatial_features, motion_features\n        )\n\n        # Detect and track objects\n        objects = self.detect_and_track_objects(fused_features)\n\n        # Ground language commands in perception\n        if hasattr(self, 'current_command'):\n            grounded_objects = self.ground_language_in_perception(\n                objects, self.current_command\n            )\n        else:\n            grounded_objects = objects\n\n        # Publish perception results\n        self.publish_perception_results(grounded_objects)\n\n    def process_visual_data(self, image):\n        \"\"\"Process visual data to extract features.\"\"\"\n        # Run object detection\n        detections = self.object_detector.detect(image)\n\n        # Run semantic segmentation\n        segmentation = self.segmentation_model.segment(image)\n\n        # Estimate poses\n        poses = self.pose_estimator.estimate_poses(image, detections)\n\n        # Extract visual features\n        features = {\n            'detections': detections,\n            'segmentation': segmentation,\n            'poses': poses,\n            'image_features': self.extract_image_features(image)\n        }\n\n        return features\n\n    def process_spatial_data(self, laser_ranges):\n        \"\"\"Process laser range data for spatial understanding.\"\"\"\n        # Detect obstacles\n        obstacles = self.detect_obstacles(laser_ranges)\n\n        # Create occupancy grid\n        occupancy_grid = self.create_occupancy_grid(laser_ranges)\n\n        # Extract spatial features\n        features = {\n            'obstacles': obstacles,\n            'occupancy_grid': occupancy_grid,\n            'free_space': self.find_free_space(laser_ranges)\n        }\n\n        return features\n\n    def detect_and_track_objects(self, fused_features):\n        \"\"\"Detect and track objects using fused features.\"\"\"\n        # Combine visual and spatial detections\n        visual_detections = fused_features['visual']['detections']\n        spatial_detections = fused_features['spatial']['obstacles']\n\n        # Associate detections across modalities\n        associated_objects = self.associate_detections(\n            visual_detections, spatial_detections\n        )\n\n        # Track objects over time\n        tracked_objects = self.track_objects(associated_objects)\n\n        return tracked_objects\n\n    def ground_language_in_perception(self, objects, command):\n        \"\"\"Ground language command in current perception.\"\"\"\n        # Parse command to extract target object description\n        target_description = self.language_processor.parse_command(command)\n\n        # Find objects matching description\n        matching_objects = [\n            obj for obj in objects\n            if self.matches_description(obj, target_description)\n        ]\n\n        # Add grounding confidence\n        for obj in matching_objects:\n            obj['grounding_confidence'] = self.calculate_grounding_confidence(\n                obj, target_description\n            )\n\n        return matching_objects\n\n    def publish_perception_results(self, objects):\n        \"\"\"Publish perception results.\"\"\"\n        # Create perception message\n        perception_msg = {\n            'timestamp': time.time(),\n            'objects': objects,\n            'confidence': np.mean([obj.get('confidence', 0) for obj in objects]) if objects else 0\n        }\n\n        # Publish as JSON string\n        self.perception_pub.publish(String(data=str(perception_msg)))\n\n        # Also publish objects separately\n        objects_msg = {'objects': objects}\n        self.objects_pub.publish(String(data=str(objects_msg)))\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-planning-system",children:"3. Planning System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Complete planning system with task and motion planning\nclass CompletePlanningSystem(Node):\n    def __init__(self):\n        super().__init__('complete_planning_system')\n\n        # Subscribers\n        self.perception_sub = self.create_subscription(String, '/perception/results', self.perception_callback, 10)\n        self.command_sub = self.create_subscription(String, '/command', self.command_callback, 10)\n\n        # Publishers\n        self.task_plan_pub = self.create_publisher(String, '/planning/task_plan', 10)\n        self.motion_plan_pub = self.create_publisher(String, '/planning/motion_plan', 10)\n\n        # Initialize planning components\n        self.task_planner = TaskPlanner()\n        self.motion_planner = MotionPlanner()\n        self.world_model = WorldModel()\n\n        # Current state\n        self.current_objects = []\n        self.current_command = None\n        self.current_goal = None\n\n        self.get_logger().info(\"Complete planning system initialized\")\n\n    def perception_callback(self, msg):\n        \"\"\"Process perception results.\"\"\"\n        try:\n            perception_data = eval(msg.data)  # In real system, use proper JSON parsing\n            self.current_objects = perception_data.get('objects', [])\n\n            # Update world model with new perception\n            self.world_model.update_objects(self.current_objects)\n\n            # If we have a command, replan\n            if self.current_command:\n                self.plan_for_command(self.current_command)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing perception: {e}\")\n\n    def command_callback(self, msg):\n        \"\"\"Process high-level command.\"\"\"\n        self.current_command = msg.data\n        self.plan_for_command(msg.data)\n\n    def plan_for_command(self, command):\n        \"\"\"Generate complete plan for command.\"\"\"\n        # Parse command and extract goal\n        goal = self.task_planner.parse_command_to_goal(command, self.world_model)\n        self.current_goal = goal\n\n        if goal is None:\n            self.get_logger().error(f\"Could not parse goal from command: {command}\")\n            return\n\n        # Create task plan\n        task_plan = self.task_planner.create_plan(goal, self.world_model)\n\n        if task_plan is None:\n            self.get_logger().error(f\"Could not create task plan for goal: {goal}\")\n            return\n\n        # Create motion plan for each task\n        complete_plan = {\n            'task_plan': task_plan,\n            'motion_plans': [],\n            'execution_sequence': []\n        }\n\n        for task in task_plan.tasks:\n            motion_plan = self.motion_planner.create_motion_plan(task, self.world_model)\n            if motion_plan:\n                complete_plan['motion_plans'].append(motion_plan)\n                complete_plan['execution_sequence'].append({\n                    'task': task,\n                    'motion_plan': motion_plan\n                })\n\n        # Publish complete plan\n        self.publish_complete_plan(complete_plan)\n\n    def publish_complete_plan(self, plan):\n        \"\"\"Publish the complete plan.\"\"\"\n        plan_msg = String()\n        plan_msg.data = str(plan)\n\n        self.task_plan_pub.publish(plan_msg)\n        self.motion_plan_pub.publish(plan_msg)\n\nclass TaskPlanner:\n    def __init__(self):\n        # Initialize task planning models\n        self.task_decomposer = TaskDecomposer()\n        self.resource_allocator = ResourceAllocator()\n        self.constraint_checker = ConstraintChecker()\n\n    def parse_command_to_goal(self, command, world_model):\n        \"\"\"Parse natural language command to specific goal.\"\"\"\n        # Use NLP to extract goal\n        parsed = self.nlp_parse_command(command)\n\n        if parsed['action'] == 'navigate':\n            return {\n                'type': 'navigation',\n                'target_location': self.find_location(parsed['target'], world_model),\n                'constraints': parsed.get('constraints', {})\n            }\n        elif parsed['action'] == 'manipulate':\n            return {\n                'type': 'manipulation',\n                'target_object': self.find_object(parsed['target'], world_model),\n                'action': parsed['action_type'],\n                'constraints': parsed.get('constraints', {})\n            }\n        # Add more action types as needed\n\n        return None\n\n    def create_plan(self, goal, world_model):\n        \"\"\"Create task plan for goal.\"\"\"\n        # Decompose goal into subtasks\n        subtasks = self.task_decomposer.decompose(goal, world_model)\n\n        # Check constraints and feasibility\n        if not self.constraint_checker.validate_tasks(subtasks, world_model):\n            return None\n\n        # Allocate resources\n        allocated_tasks = self.resource_allocator.allocate(subtasks, world_model)\n\n        # Create plan with temporal and causal relationships\n        plan = Plan()\n        plan.tasks = allocated_tasks\n        plan.dependencies = self.calculate_dependencies(allocated_tasks)\n\n        return plan\n\nclass MotionPlanner:\n    def __init__(self):\n        # Initialize motion planning algorithms\n        self.path_planner = PathPlanner()\n        self.trajectory_planner = TrajectoryPlanner()\n        self.ik_solver = InverseKinematicsSolver()\n\n    def create_motion_plan(self, task, world_model):\n        \"\"\"Create motion plan for a task.\"\"\"\n        if task.type == 'navigation':\n            return self.plan_navigation(task, world_model)\n        elif task.type == 'manipulation':\n            return self.plan_manipulation(task, world_model)\n        # Add more task types\n\n        return None\n\n    def plan_navigation(self, task, world_model):\n        \"\"\"Plan navigation motion.\"\"\"\n        # Get start and goal positions\n        start_pos = world_model.get_robot_position()\n        goal_pos = task.target_location\n\n        # Plan collision-free path\n        path = self.path_planner.plan_path(start_pos, goal_pos, world_model.get_occupancy_grid())\n\n        if path is None:\n            return None\n\n        # Generate smooth trajectory\n        trajectory = self.trajectory_planner.generate_trajectory(path, world_model)\n\n        return {\n            'type': 'navigation',\n            'path': path,\n            'trajectory': trajectory,\n            'waypoints': self.extract_waypoints(path)\n        }\n\n    def plan_manipulation(self, task, world_model):\n        \"\"\"Plan manipulation motion.\"\"\"\n        # Get object position and robot end-effector constraints\n        object_pos = world_model.get_object_position(task.target_object)\n        robot_config = world_model.get_robot_configuration()\n\n        # Plan approach, grasp, and manipulation trajectory\n        approach_poses = self.calculate_approach_poses(object_pos, task.action)\n        grasp_poses = self.calculate_grasp_poses(object_pos, task.target_object)\n\n        # Solve inverse kinematics\n        joint_trajectories = []\n        for pose in approach_poses + grasp_poses:\n            joint_config = self.ik_solver.solve(pose, robot_config)\n            if joint_config is not None:\n                joint_trajectories.append(joint_config)\n\n        return {\n            'type': 'manipulation',\n            'approach_poses': approach_poses,\n            'grasp_poses': grasp_poses,\n            'joint_trajectories': joint_trajectories\n        }\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-action-system",children:"4. Action System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Complete action execution system\nclass CompleteActionSystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_action_system\')\n\n        # Subscribers\n        self.task_plan_sub = self.create_subscription(String, \'/planning/task_plan\', self.task_plan_callback, 10)\n        self.motion_plan_sub = self.create_subscription(String, \'/planning/motion_plan\', self.motion_plan_callback, 10)\n\n        # Publishers for robot commands\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\n\n        # Initialize action executors\n        self.task_executor = TaskExecutor()\n        self.motion_executor = MotionExecutor()\n        self.safety_monitor = SafetyMonitor()\n\n        # Execution state\n        self.current_plan = None\n        self.execution_active = False\n        self.execution_thread = None\n\n        self.get_logger().info("Complete action system initialized")\n\n    def task_plan_callback(self, msg):\n        """Execute task plan."""\n        try:\n            plan_data = eval(msg.data)  # In real system, use proper JSON parsing\n            self.execute_task_plan(plan_data)\n        except Exception as e:\n            self.get_logger().error(f"Error executing task plan: {e}")\n\n    def motion_plan_callback(self, msg):\n        """Execute motion plan."""\n        try:\n            plan_data = eval(msg.data)  # In real system, use proper JSON parsing\n            self.execute_motion_plan(plan_data)\n        except Exception as e:\n            self.get_logger().error(f"Error executing motion plan: {e}")\n\n    def execute_task_plan(self, plan_data):\n        """Execute complete task plan."""\n        # Stop any currently executing plan\n        self.stop_execution()\n\n        # Validate plan safety\n        if not self.safety_monitor.validate_plan(plan_data):\n            self.get_logger().error("Plan failed safety validation")\n            return\n\n        # Execute plan in separate thread\n        self.current_plan = plan_data\n        self.execution_active = True\n\n        self.execution_thread = threading.Thread(target=self.execute_plan_thread, args=(plan_data,))\n        self.execution_thread.start()\n\n    def execute_plan_thread(self, plan_data):\n        """Execute plan in separate thread."""\n        try:\n            # Execute each task in sequence\n            for execution_item in plan_data[\'execution_sequence\']:\n                task = execution_item[\'task\']\n                motion_plan = execution_item[\'motion_plan\']\n\n                # Execute motion plan\n                success = self.execute_motion_plan(motion_plan)\n\n                if not success:\n                    self.get_logger().error(f"Failed to execute motion plan for task: {task}")\n                    break\n\n                # Check for safety violations\n                if self.safety_monitor.safety_violation_detected():\n                    self.emergency_stop()\n                    break\n\n            self.execution_active = False\n\n        except Exception as e:\n            self.get_logger().error(f"Error in execution thread: {e}")\n            self.emergency_stop()\n\n    def execute_motion_plan(self, motion_plan):\n        """Execute a motion plan."""\n        if motion_plan[\'type\'] == \'navigation\':\n            return self.execute_navigation(motion_plan)\n        elif motion_plan[\'type\'] == \'manipulation\':\n            return self.execute_manipulation(motion_plan)\n\n        return False\n\n    def execute_navigation(self, motion_plan):\n        """Execute navigation motion plan."""\n        # Follow trajectory waypoint by waypoint\n        for waypoint in motion_plan[\'waypoints\']:\n            # Generate velocity commands to reach waypoint\n            cmd_vel = self.calculate_navigation_command(waypoint)\n\n            # Publish command\n            self.cmd_vel_pub.publish(cmd_vel)\n\n            # Wait for robot to reach waypoint (with timeout)\n            if not self.wait_for_waypoint_reached(waypoint, timeout=10.0):\n                return False\n\n            # Check safety\n            if self.safety_monitor.safety_violation_detected():\n                self.emergency_stop()\n                return False\n\n        return True\n\n    def execute_manipulation(self, motion_plan):\n        """Execute manipulation motion plan."""\n        # Execute joint trajectory\n        for joint_config in motion_plan[\'joint_trajectories\']:\n            # Create joint command\n            joint_cmd = JointState()\n            joint_cmd.name = [f\'joint_{i}\' for i in range(len(joint_config))]\n            joint_cmd.position = joint_config\n            joint_cmd.velocity = [0.0] * len(joint_config)  # Start with zero velocity\n\n            # Publish joint command\n            self.joint_cmd_pub.publish(joint_cmd)\n\n            # Wait for joint movement to complete\n            if not self.wait_for_joints_reached(joint_config, timeout=5.0):\n                return False\n\n            # Check safety\n            if self.safety_monitor.safety_violation_detected():\n                self.emergency_stop()\n                return False\n\n        return True\n\n    def calculate_navigation_command(self, waypoint):\n        """Calculate velocity command to reach waypoint."""\n        # Simple proportional controller\n        cmd = Twist()\n\n        # Calculate error to waypoint\n        pos_error = np.linalg.norm(waypoint[:2] - self.get_current_position()[:2])\n        angle_error = waypoint[2] - self.get_current_position()[2]\n\n        # Proportional control\n        cmd.linear.x = min(0.5, pos_error * 0.5)  # Max 0.5 m/s\n        cmd.angular.z = min(0.5, angle_error * 1.0)  # Max 0.5 rad/s\n\n        return cmd\n\n    def wait_for_waypoint_reached(self, waypoint, timeout=10.0):\n        """Wait for robot to reach waypoint."""\n        start_time = time.time()\n        tolerance = 0.1  # 10cm tolerance\n\n        while time.time() - start_time < timeout:\n            current_pos = self.get_current_position()\n            distance = np.linalg.norm(waypoint[:2] - current_pos[:2])\n\n            if distance < tolerance:\n                return True\n\n            time.sleep(0.1)\n\n        return False\n\n    def wait_for_joints_reached(self, target_positions, timeout=5.0):\n        """Wait for joints to reach target positions."""\n        start_time = time.time()\n        tolerance = 0.01  # 1cm tolerance\n\n        while time.time() - start_time < timeout:\n            current_positions = self.get_current_joint_positions()\n\n            # Check if all joints are within tolerance\n            errors = np.abs(np.array(target_positions) - np.array(current_positions))\n            if all(error < tolerance for error in errors):\n                return True\n\n            time.sleep(0.1)\n\n        return False\n\n    def get_current_position(self):\n        """Get current robot position (in simulation context)."""\n        # In real implementation, subscribe to odometry\n        return np.array([0.0, 0.0, 0.0])  # Placeholder\n\n    def get_current_joint_positions(self):\n        """Get current joint positions (in simulation context)."""\n        # In real implementation, subscribe to joint states\n        return [0.0] * 32  # Placeholder for 32 DoF\n\n    def stop_execution(self):\n        """Stop current execution."""\n        self.execution_active = False\n\n        if self.execution_thread and self.execution_thread.is_alive():\n            self.execution_thread.join(timeout=1.0)\n\n        # Send stop commands to robot\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n    def emergency_stop(self):\n        """Execute emergency stop."""\n        self.stop_execution()\n\n        # Publish emergency stop command\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n        self.get_logger().warn("Emergency stop executed!")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"5-safety-system-integration",children:"5. Safety System Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Complete safety system for the entire pipeline\nclass CompleteSafetySystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_safety_system\')\n\n        # Subscribers for monitoring all pipeline stages\n        self.perception_sub = self.create_subscription(String, \'/perception/results\', self.perception_monitor, 10)\n        self.plan_sub = self.create_subscription(String, \'/planning/task_plan\', self.plan_monitor, 10)\n        self.cmd_sub = self.create_subscription(Twist, \'/cmd_vel\', self.command_monitor, 10)\n\n        # Publisher for safety commands\n        self.emergency_stop_pub = self.create_publisher(Bool, \'/emergency_stop\', 10)\n\n        # Initialize safety components\n        self.collision_detector = CollisionDetector()\n        self.human_proximity_monitor = HumanProximityMonitor()\n        self.velocity_limiter = VelocityLimiter()\n\n        # Safety state\n        self.safety_violation = False\n        self.last_safe_time = time.time()\n\n        # Start safety monitoring\n        self.safety_timer = self.create_timer(0.1, self.safety_check)  # 10Hz safety check\n\n        self.get_logger().info("Complete safety system initialized")\n\n    def perception_monitor(self, msg):\n        """Monitor perception data for safety issues."""\n        try:\n            perception_data = eval(msg.data)\n            objects = perception_data.get(\'objects\', [])\n\n            # Check for humans in proximity\n            humans_nearby = [obj for obj in objects if obj.get(\'type\') == \'human\']\n\n            if humans_nearby:\n                self.check_human_safety(humans_nearby)\n\n            # Check for obstacles in path\n            obstacles = [obj for obj in objects if obj.get(\'type\') == \'obstacle\']\n            self.check_obstacle_safety(obstacles)\n\n        except Exception as e:\n            self.get_logger().error(f"Safety perception monitoring error: {e}")\n\n    def plan_monitor(self, msg):\n        """Monitor plans for safety issues."""\n        try:\n            plan_data = eval(msg.data)\n\n            # Check if plan is safe\n            if not self.validate_plan_safety(plan_data):\n                self.trigger_safety_violation("Unsafe plan detected")\n\n        except Exception as e:\n            self.get_logger().error(f"Safety plan monitoring error: {e}")\n\n    def command_monitor(self, msg):\n        """Monitor commands for safety issues."""\n        # Check command velocity limits\n        if abs(msg.linear.x) > 1.0 or abs(msg.angular.z) > 1.0:\n            self.trigger_safety_violation("Excessive velocity command")\n\n        # Check for dangerous command patterns\n        self.check_command_patterns(msg)\n\n    def safety_check(self):\n        """Regular safety check."""\n        # Check system health\n        if not self.system_health_check():\n            self.trigger_safety_violation("System health issue")\n\n        # Check sensor validity\n        if not self.sensors_valid():\n            self.trigger_safety_violation("Invalid sensor data")\n\n        # Check timing constraints\n        if time.time() - self.last_safe_time > 5.0:  # 5 seconds without safety check\n            self.trigger_safety_violation("Safety system timeout")\n\n    def check_human_safety(self, humans):\n        """Check safety regarding humans."""\n        for human in humans:\n            distance = self.calculate_distance_to_robot(human)\n\n            if distance < 1.0:  # Less than 1 meter\n                self.trigger_safety_violation(f"Human too close: {distance:.2f}m")\n                return\n\n    def validate_plan_safety(self, plan_data):\n        """Validate that a plan is safe to execute."""\n        # Check navigation plan for collisions\n        if \'execution_sequence\' in plan_data:\n            for item in plan_data[\'execution_sequence\']:\n                if item[\'task\'][\'type\'] == \'navigation\':\n                    path = item[\'motion_plan\'].get(\'path\', [])\n                    if not self.path_is_safe(path):\n                        return False\n\n        return True\n\n    def path_is_safe(self, path):\n        """Check if a navigation path is safe."""\n        # Check path for obstacles\n        for point in path:\n            if self.collision_detector.would_collide_at(point):\n                return False\n\n        return True\n\n    def trigger_safety_violation(self, reason):\n        """Trigger safety violation and emergency stop."""\n        self.safety_violation = True\n        self.last_safe_time = time.time()\n\n        self.get_logger().error(f"Safety violation: {reason}")\n\n        # Publish emergency stop\n        emergency_msg = Bool()\n        emergency_msg.data = True\n        self.emergency_stop_pub.publish(emergency_msg)\n\n    def system_health_check(self):\n        """Check overall system health."""\n        # In real implementation, check all subsystems\n        return True\n\n    def sensors_valid(self):\n        """Check if sensor data is valid."""\n        # In real implementation, validate sensor streams\n        return True\n'})}),"\n",(0,s.jsx)(n.h2,{id:"complete-integration-example",children:"Complete Integration Example"}),"\n",(0,s.jsx)(n.h3,{id:"main-pipeline-node",children:"Main Pipeline Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Complete integrated pipeline node\nclass CompleteSPPAPipeline(Node):\n    def __init__(self):\n        super().__init__(\'complete_sppa_pipeline\')\n\n        # Initialize all pipeline components\n        self.simulation = CompleteSimulationEnvironment()\n        self.perception = CompletePerceptionSystem()\n        self.planning = CompletePlanningSystem()\n        self.action = CompleteActionSystem()\n        self.safety = CompleteSafetySystem()\n\n        # Setup inter-component communication\n        self.setup_pipeline_connections()\n\n        # Performance monitoring\n        self.performance_monitor = PerformanceMonitor()\n\n        # Start performance monitoring\n        self.performance_timer = self.create_timer(1.0, self.performance_check)\n\n        self.get_logger().info("Complete SPPA pipeline initialized")\n\n    def setup_pipeline_connections(self):\n        """Setup connections between pipeline components."""\n        # In ROS 2, connections are made through topics\n        # All components are already connected via topic names\n        pass\n\n    def performance_check(self):\n        """Check pipeline performance metrics."""\n        metrics = self.performance_monitor.get_metrics()\n\n        # Log performance metrics\n        self.get_logger().info(f"Pipeline Performance - Perception: {metrics[\'perception\']:.3f}s, "\n                              f"Planning: {metrics[\'planning\']:.3f}s, "\n                              f"Action: {metrics[\'action\']:.3f}s")\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = {\n            \'perception_time\': [],\n            \'planning_time\': [],\n            \'action_time\': [],\n            \'total_cycle_time\': []\n        }\n        self.start_times = {}\n\n    def start_timer(self, component):\n        """Start timer for a component."""\n        self.start_times[component] = time.time()\n\n    def stop_timer(self, component):\n        """Stop timer for a component and record metric."""\n        if component in self.start_times:\n            elapsed = time.time() - self.start_times[component]\n            self.metrics[f\'{component}_time\'].append(elapsed)\n\n            # Keep only last 100 measurements\n            if len(self.metrics[f\'{component}_time\']) > 100:\n                self.metrics[f\'{component}_time\'].pop(0)\n\n    def get_metrics(self):\n        """Get current performance metrics."""\n        result = {}\n        for key, values in self.metrics.items():\n            if values:\n                result[key] = sum(values) / len(values)  # Average\n            else:\n                result[key] = 0.0\n        return result\n'})}),"\n",(0,s.jsx)(n.h2,{id:"real-world-deployment-example",children:"Real-World Deployment Example"}),"\n",(0,s.jsx)(n.h3,{id:"deployment-configuration",children:"Deployment Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Complete deployment configuration for SPPA pipeline\ndeployment_config:\n  pipeline:\n    simulate:\n      environment: "real_world"\n      physics_engine: "bullet"\n      update_rate: 100  # Hz\n      objects:\n        - name: "table"\n          type: "furniture"\n          static: true\n        - name: "ball"\n          type: "movable"\n          dynamic: true\n\n    perceive:\n      sensors:\n        camera:\n          resolution: "1920x1080"\n          frame_rate: 30\n          fov: 60\n        lidar:\n          range: 20.0\n          resolution: 0.01\n          frame_rate: 10\n        imu:\n          rate: 100\n          precision: "high"\n      processing:\n        detection_threshold: 0.7\n        tracking_iou_threshold: 0.5\n        fusion_confidence_weight: 0.8\n\n    plan:\n      task_planning:\n        decomposition_depth: 5\n        resource_constraints: true\n        temporal_logic: true\n      motion_planning:\n        path_planner: "teb"\n        trajectory_planner: "polynomial"\n        collision_check_resolution: 0.05\n\n    act:\n      control_frequency: 100  # Hz\n      trajectory_tracking:\n        position_tolerance: 0.05  # 5cm\n        orientation_tolerance: 0.1  # 0.1 rad\n      safety:\n        emergency_stop_response_time: 0.1  # 100ms\n        collision_avoidance_distance: 0.5  # 50cm\n\n  safety:\n    collision_threshold: 0.5  # meters\n    human_proximity_threshold: 1.0  # meters\n    velocity_limits:\n      linear: 1.0  # m/s\n      angular: 1.0  # rad/s\n    emergency_procedures:\n      - "stop_immediately"\n      - "return_to_home"\n      - "wait_for_reset"\n\n  performance:\n    real_time_requirements:\n      perception: 0.1  # 100ms\n      planning: 0.2    # 200ms\n      action: 0.01     # 10ms\n    resource_limits:\n      cpu: 80  # percent\n      memory: 85  # percent\n      gpu: 85  # percent\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing-the-complete-pipeline",children:"Testing the Complete Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"integration-test",children:"Integration Test"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example integration test for complete pipeline\nimport unittest\nimport time\nfrom unittest.mock import Mock, patch\n\nclass TestCompleteSPPAPipeline(unittest.TestCase):\n    def setUp(self):\n        """Set up complete pipeline for testing."""\n        self.pipeline = CompleteSPPAPipeline()\n        self.test_environment = TestEnvironment()\n\n    def test_complete_pipeline_operation(self):\n        """Test complete pipeline from command to action."""\n        # Setup: Clear environment with known objects\n        self.test_environment.add_object(\'target_ball\', position=(2.0, 1.0, 0.0))\n        self.test_environment.set_robot_position((0.0, 0.0, 0.0))\n\n        # Action: Send command to pipeline\n        command = "Go to the red ball and pick it up"\n\n        # Simulate sending command through the pipeline\n        self.simulate_command_input(command)\n\n        # Wait for pipeline to process\n        time.sleep(5.0)  # Allow time for complete processing\n\n        # Verify: Check if robot reached target\n        final_position = self.test_environment.get_robot_position()\n\n        # Should be close to target ball position\n        target_distance = self.calculate_distance(final_position, (2.0, 1.0, 0.0))\n        self.assertLess(target_distance, 0.5)  # Within 50cm\n\n        # Verify: Check that manipulation occurred\n        self.assertTrue(self.test_environment.object_moved(\'target_ball\'))\n\n    def test_safety_in_pipeline(self):\n        """Test safety throughout pipeline operation."""\n        # Setup: Environment with human nearby\n        self.test_environment.add_object(\'human\', position=(1.0, 0.5, 0.0))\n        self.test_environment.set_robot_position((0.0, 0.0, 0.0))\n\n        # Action: Send command that would approach human\n        command = "Move toward position (1.5, 0.5)"\n\n        # Simulate sending command\n        self.simulate_command_input(command)\n\n        # Wait briefly\n        time.sleep(2.0)\n\n        # Verify: Robot should maintain safe distance from human\n        robot_pos = self.test_environment.get_robot_position()\n        human_pos = (1.0, 0.5, 0.0)\n        distance = self.calculate_distance(robot_pos, human_pos)\n\n        # Should maintain at least 1m safety distance\n        self.assertGreater(distance, 1.0)\n\n    def test_pipeline_performance(self):\n        """Test pipeline performance under load."""\n        start_time = time.time()\n\n        # Run multiple commands in succession\n        commands = [\n            "Navigate to kitchen",\n            "Pick up object",\n            "Navigate to living room",\n            "Place object"\n        ]\n\n        for command in commands:\n            self.simulate_command_input(command)\n            time.sleep(0.5)  # Brief pause between commands\n\n        total_time = time.time() - start_time\n\n        # Should complete 4 commands in reasonable time\n        self.assertLess(total_time, 10.0)  # Less than 10 seconds\n\n    def simulate_command_input(self, command):\n        """Simulate sending command to pipeline."""\n        # In real test, this would publish to ROS topic\n        pass\n\n    def calculate_distance(self, pos1, pos2):\n        """Calculate 2D distance between positions."""\n        dx = pos1[0] - pos2[0]\n        dy = pos1[1] - pos2[1]\n        return (dx**2 + dy**2)**0.5\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"cross-references",children:"Cross-References"}),"\n",(0,s.jsx)(n.p,{children:"For related concepts, see:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/ros2/implementation",children:"ROS 2 Integration"})," for communication patterns [31]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/digital-twin/integration",children:"Digital Twin Integration"})," for simulation connections [32]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/nvidia-isaac/examples",children:"NVIDIA Isaac Integration"})," for GPU acceleration [33]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/vla-systems/implementation",children:"VLA Integration"})," for multimodal systems [34]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/hardware-guide/sensors",children:"Hardware Integration"})," for deployment [35]"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.p,{children:['[1] Complete Pipeline. (2023). "SPPA System Integration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[2] Perception Planning Action. (2023). "Integrated Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001234",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001234"})]}),"\n",(0,s.jsxs)(n.p,{children:['[3] Pipeline Integration. (2023). "System Integration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[4] Real-time Processing. (2023). "Pipeline Performance". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001246",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001246"})]}),"\n",(0,s.jsxs)(n.p,{children:['[5] Safety Integration. (2023). "Pipeline Safety". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[6] Performance Optimization. (2023). "Pipeline Optimization". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001258",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001258"})]}),"\n",(0,s.jsxs)(n.p,{children:['[7] Debugging Pipeline. (2023). "Pipeline Debugging". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[8] Pipeline Validation. (2023). "System Validation". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S240545262100126X",children:"https://www.sciencedirect.com/science/article/pii/S240545262100126X"})]}),"\n",(0,s.jsxs)(n.p,{children:['[9] Performance Metrics. (2023). "Pipeline Metrics". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[10] Documentation. (2023). "Pipeline Documentation". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001271",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001271"})]}),"\n",(0,s.jsxs)(n.p,{children:['[11] Simulation Integration. (2023). "Sim Environment". Retrieved from ',(0,s.jsx)(n.a,{href:"https://gazebosim.org/",children:"https://gazebosim.org/"})]}),"\n",(0,s.jsxs)(n.p,{children:['[12] Perception System. (2023). "Multimodal Perception". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[13] Planning System. (2023). "Integrated Planning". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001283",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001283"})]}),"\n",(0,s.jsxs)(n.p,{children:['[14] Action System. (2023). "Action Execution". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[15] Safety System. (2023). "Safety Integration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001295",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001295"})]}),"\n",(0,s.jsxs)(n.p,{children:['[16] System Architecture. (2023). "Pipeline Architecture". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[17] Component Integration. (2023). "System Integration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001301",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001301"})]}),"\n",(0,s.jsxs)(n.p,{children:['[18] Real-time Processing. (2023). "Timing Constraints". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[19] Performance Monitoring. (2023). "System Monitoring". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001313",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001313"})]}),"\n",(0,s.jsxs)(n.p,{children:['[20] Safety Validation. (2023). "Safety Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[21] Deployment Configuration. (2023). "System Configuration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001325",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001325"})]}),"\n",(0,s.jsxs)(n.p,{children:['[22] Integration Testing. (2023). "System Testing". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[23] Performance Tuning. (2023). "System Optimization". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001337",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001337"})]}),"\n",(0,s.jsxs)(n.p,{children:['[24] Pipeline Architecture. (2023). "System Design". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[25] Component Connection. (2023). "System Integration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001349",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001349"})]}),"\n",(0,s.jsxs)(n.p,{children:['[26] Real-time Requirements. (2023). "Timing Constraints". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[27] Performance Metrics. (2023). "System Metrics". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001350",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001350"})]}),"\n",(0,s.jsxs)(n.p,{children:['[28] Safety Integration. (2023). "Safety Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[29] Configuration Management. (2023). "System Configuration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001362",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001362"})]}),"\n",(0,s.jsxs)(n.p,{children:['[30] Testing Procedures. (2023). "System Testing". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[31] ROS Integration. (2023). "Communication Patterns". Retrieved from ',(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,s.jsxs)(n.p,{children:['[32] Simulation Connection. (2023). "Integration Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://gazebosim.org/",children:"https://gazebosim.org/"})]}),"\n",(0,s.jsxs)(n.p,{children:['[33] GPU Integration. (2023). "Acceleration Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/",children:"https://docs.nvidia.com/isaac/"})]}),"\n",(0,s.jsxs)(n.p,{children:['[34] Multimodal Integration. (2023). "VLA Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2306.17100",children:"https://arxiv.org/abs/2306.17100"})]}),"\n",(0,s.jsxs)(n.p,{children:['[35] Hardware Integration. (2023). "Deployment Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001374",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001374"})]}),"\n",(0,s.jsxs)(n.p,{children:['[36] Pipeline Components. (2023). "System Components". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[37] System Integration. (2023). "Integration Procedures". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001386",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001386"})]}),"\n",(0,s.jsxs)(n.p,{children:['[38] Real-time Processing. (2023). "Processing Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[39] Performance Monitoring. (2023). "Monitoring Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001398",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001398"})]}),"\n",(0,s.jsxs)(n.p,{children:['[40] Safety Systems. (2023). "Safety Procedures". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[41] Configuration Files. (2023). "System Configuration". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001404",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001404"})]}),"\n",(0,s.jsxs)(n.p,{children:['[42] Integration Testing. (2023). "Testing Procedures". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[43] Performance Tuning. (2023). "Optimization Procedures". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001416",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001416"})]}),"\n",(0,s.jsxs)(n.p,{children:['[44] Architecture Design. (2023). "System Architecture". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[45] Component Connection. (2023). "Integration Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001428",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001428"})]}),"\n",(0,s.jsxs)(n.p,{children:['[46] Timing Constraints. (2023). "Real-time Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[47] System Metrics. (2023). "Performance Metrics". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S240545262100143X",children:"https://www.sciencedirect.com/science/article/pii/S240545262100143X"})]}),"\n",(0,s.jsxs)(n.p,{children:['[48] Safety Procedures. (2023). "Safety Systems". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,s.jsxs)(n.p,{children:['[49] Configuration Management. (2023). "System Setup". Retrieved from ',(0,s.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001441",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001441"})]}),"\n",(0,s.jsxs)(n.p,{children:['[50] Testing Systems. (2023). "Validation Procedures". Retrieved from ',(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}}}]);