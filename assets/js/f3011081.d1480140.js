"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[6193],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>c});var r=i(6540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},9736:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"hardware-guide/integration-diagrams-exercises","title":"Hardware-Software Integration Diagrams and Exercises","description":"Diagrams and exercises connecting hardware components with software systems for humanoid robotics applications","source":"@site/docs/hardware-guide/integration-diagrams-exercises.md","sourceDirName":"hardware-guide","slug":"/hardware-guide/integration-diagrams-exercises","permalink":"/Physical-AI-Robotics-Book/docs/hardware-guide/integration-diagrams-exercises","draft":false,"unlisted":false,"editUrl":"https://github.com/Muneeb-Ahmed-Github-Account/Physical-AI-Robotics-Book/tree/main/docs/hardware-guide/integration-diagrams-exercises.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Hardware-Software Integration Diagrams and Exercises","sidebar_position":6,"description":"Diagrams and exercises connecting hardware components with software systems for humanoid robotics applications"}}');var t=i(4848),s=i(8453);const o={title:"Hardware-Software Integration Diagrams and Exercises",sidebar_position:6,description:"Diagrams and exercises connecting hardware components with software systems for humanoid robotics applications"},c="Hardware-Software Integration Diagrams and Exercises",a={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"System Architecture Diagrams",id:"system-architecture-diagrams",level:2},{value:"Overall Hardware-Software Architecture",id:"overall-hardware-software-architecture",level:3},{value:"Sensor-Software Integration Pattern",id:"sensor-software-integration-pattern",level:3},{value:"Actuator-Software Integration Pattern",id:"actuator-software-integration-pattern",level:3},{value:"Real-Time Integration Architecture",id:"real-time-integration-architecture",level:3},{value:"Hardware-Software Integration Patterns",id:"hardware-software-integration-patterns",level:3},{value:"Communication Protocol Mapping",id:"communication-protocol-mapping",level:3},{value:"System Architecture Integration Pattern",id:"system-architecture-integration-pattern",level:3},{value:"Hardware-Software Integration Exercises",id:"hardware-software-integration-exercises",level:2},{value:"Exercise 1: Camera-Sensor Integration",id:"exercise-1-camera-sensor-integration",level:3},{value:"Objective",id:"objective",level:4},{value:"Scenario",id:"scenario",level:4},{value:"Steps",id:"steps",level:4},{value:"Exercise 2: IMU-Integration for Balance Control",id:"exercise-2-imu-integration-for-balance-control",level:3},{value:"Objective",id:"objective-1",level:4},{value:"Scenario",id:"scenario-1",level:4},{value:"Steps",id:"steps-1",level:4},{value:"Exercise 3: Multi-Sensor Fusion Exercise",id:"exercise-3-multi-sensor-fusion-exercise",level:3},{value:"Objective",id:"objective-2",level:4},{value:"Scenario",id:"scenario-2",level:4},{value:"Implementation",id:"implementation",level:4},{value:"Hardware-Software Integration Exercises",id:"hardware-software-integration-exercises-1",level:2},{value:"Exercise 1: Camera-Sensor Integration",id:"exercise-1-camera-sensor-integration-1",level:3},{value:"Objective",id:"objective-3",level:4},{value:"Scenario",id:"scenario-3",level:4},{value:"Steps",id:"steps-2",level:4},{value:"Exercise 2: IMU-Integration for Balance Control",id:"exercise-2-imu-integration-for-balance-control-1",level:3},{value:"Objective",id:"objective-4",level:4},{value:"Scenario",id:"scenario-4",level:4},{value:"Steps",id:"steps-3",level:4},{value:"Exercise 3: Multi-Sensor Fusion Exercise",id:"exercise-3-multi-sensor-fusion-exercise-1",level:3},{value:"Objective",id:"objective-5",level:4},{value:"Scenario",id:"scenario-5",level:4},{value:"Implementation",id:"implementation-1",level:4},{value:"Integration Best Practices",id:"integration-best-practices",level:2},{value:"1. Data Synchronization",id:"1-data-synchronization",level:3},{value:"2. Calibration",id:"2-calibration",level:3},{value:"3. Fault Tolerance",id:"3-fault-tolerance",level:3},{value:"4. Real-Time Considerations",id:"4-real-time-considerations",level:3},{value:"Troubleshooting Integration Issues",id:"troubleshooting-integration-issues",level:2},{value:"Common Problems and Solutions",id:"common-problems-and-solutions",level:3},{value:"Cross-References",id:"cross-references",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"hardware-software-integration-diagrams-and-exercises",children:"Hardware-Software Integration Diagrams and Exercises"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this integration guide, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the relationship between hardware components and software systems [1]"}),"\n",(0,t.jsx)(n.li,{children:"Interpret system architecture diagrams for humanoid robots [2]"}),"\n",(0,t.jsx)(n.li,{children:"Design hardware-software integration patterns [3]"}),"\n",(0,t.jsx)(n.li,{children:"Connect physical sensors to perception software [4]"}),"\n",(0,t.jsx)(n.li,{children:"Integrate actuators with control software [5]"}),"\n",(0,t.jsx)(n.li,{children:"Validate hardware-software interfaces [6]"}),"\n",(0,t.jsx)(n.li,{children:"Troubleshoot integration issues [7]"}),"\n",(0,t.jsx)(n.li,{children:"Plan integration workflows [8]"}),"\n",(0,t.jsx)(n.li,{children:"Apply integration concepts to real-world scenarios [9]"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate integration effectiveness [10]"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture-diagrams",children:"System Architecture Diagrams"}),"\n",(0,t.jsx)(n.h3,{id:"overall-hardware-software-architecture",children:"Overall Hardware-Software Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     HUMANOID ROBOT SYSTEM                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502  \u2502   SENSORS   \u2502    \u2502  COMPUTING  \u2502    \u2502 ACTUATORS   \u2502             \u2502\n\u2502  \u2502             \u2502    \u2502             \u2502    \u2502             \u2502             \u2502\n\u2502  \u2502 \u2022 Cameras   \u2502    \u2502 \u2022 Jetson    \u2502    \u2502 \u2022 Servos    \u2502             \u2502\n\u2502  \u2502 \u2022 LiDAR     \u2502\u2500\u2500\u2500\u2500\u2502 \u2022 RTX GPU   \u2502\u2500\u2500\u2500\u2500\u2502 \u2022 Motors    \u2502             \u2502\n\u2502  \u2502 \u2022 IMU       \u2502    \u2502 \u2022 CPU       \u2502    \u2502 \u2022 Pneumatics\u2502             \u2502\n\u2502  \u2502 \u2022 Force/Tq  \u2502    \u2502 \u2022 RAM       \u2502    \u2502 \u2022 Hydraulics\u2502             \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502                    SOFTWARE LAYER                               \u2502\u2502\n\u2502  \u2502                                                                 \u2502\u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\u2502\n\u2502  \u2502  \u2502 PERCEPTION  \u2502 \u2502   PLANNING  \u2502 \u2502   CONTROL   \u2502               \u2502\u2502\n\u2502  \u2502  \u2502             \u2502 \u2502             \u2502 \u2502             \u2502               \u2502\u2502\n\u2502  \u2502  \u2502 \u2022 Vision    \u2502 \u2502 \u2022 Path Plan \u2502 \u2502 \u2022 Motion    \u2502               \u2502\u2502\n\u2502  \u2502  \u2502 \u2022 SLAM      \u2502 \u2502 \u2022 Task Plan \u2502 \u2502 \u2022 Balance   \u2502               \u2502\u2502\n\u2502  \u2502  \u2502 \u2022 Tracking  \u2502 \u2502 \u2022 Behavior  \u2502 \u2502 \u2022 Safety    \u2502               \u2502\u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\u2502\n\u2502  \u2502                                                                 \u2502\u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u2502\n\u2502  \u2502  \u2502                   ROS 2 FRAMEWORK                           \u2502\u2502\u2502\n\u2502  \u2502  \u2502                                                             \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u2502   NODES     \u2502 \u2502   TOPICS    \u2502 \u2502  SERVICES   \u2502           \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u2502             \u2502 \u2502             \u2502 \u2502             \u2502           \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Cam Proc  \u2502 \u2502 \u2022 /image    \u2502 \u2502 \u2022 /move     \u2502           \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Nav Stack \u2502 \u2502 \u2022 /scan     \u2502 \u2502 \u2022 /servo    \u2502           \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Control   \u2502 \u2502 \u2022 /cmd_vel  \u2502 \u2502 \u2022 /gripper  \u2502           \u2502\u2502\u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\u2502\u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"sensor-software-integration-pattern",children:"Sensor-Software Integration Pattern"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   HARDWARE      \u2502    \u2502  DRIVERS/       \u2502    \u2502   APPLICATION   \u2502\n\u2502   SENSORS       \u2502\u2500\u2500\u2500\u25b6\u2502  INTERFACES     \u2502\u2500\u2500\u2500\u25b6\u2502   LAYER         \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 \u2022 Camera        \u2502    \u2502 \u2022 sensor_msgs/  \u2502    \u2502 \u2022 Perception    \u2502\n\u2502 \u2022 LiDAR         \u2502    \u2502   Image         \u2502    \u2502 \u2022 SLAM          \u2502\n\u2502 \u2022 IMU           \u2502    \u2502 \u2022 sensor_msgs/  \u2502    \u2502 \u2022 Localization  \u2502\n\u2502 \u2022 Force/Torque  \u2502    \u2502   LaserScan     \u2502    \u2502 \u2022 Object Det.   \u2502\n\u2502 \u2022 GPS           \u2502    \u2502 \u2022 sensor_msgs/  \u2502    \u2502 \u2022 Tracking      \u2502\n\u2502 \u2022 Joint Enc.    \u2502    \u2502   Imu           \u2502    \u2502 \u2022 State Est.    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u2022 sensor_msgs/  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502   WrenchStamped \u2502\n                       \u2502 \u2022 sensor_msgs/  \u2502\n                       \u2502   JointState    \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"actuator-software-integration-pattern",children:"Actuator-Software Integration Pattern"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   APPLICATION   \u2502    \u2502  DRIVERS/       \u2502    \u2502   HARDWARE      \u2502\n\u2502   LAYER         \u2502\u2500\u2500\u2500\u25b6\u2502  INTERFACES     \u2502\u2500\u2500\u2500\u25b6\u2502   ACTUATORS     \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 \u2022 Motion Plan   \u2502    \u2502 \u2022 trajectory_   \u2502    \u2502 \u2022 Servo Motors  \u2502\n\u2502 \u2022 Path Following\u2502    \u2502   msgs/Joint    \u2502    \u2502 \u2022 Wheel Motors  \u2502\n\u2502 \u2022 Balance Ctrl  \u2502    \u2502   Trajectory    \u2502    \u2502 \u2022 Hydraulic     \u2502\n\u2502 \u2022 Grasp Control \u2502    \u2502 \u2022 geometry_msgs/\u2502    \u2502 \u2022 Pneumatic     \u2502\n\u2502 \u2022 Task Exec     \u2502    \u2502   Twist         \u2502    \u2502 \u2022 Linear Act.   \u2502\n\u2502 \u2022 Impedance Ctrl\u2502    \u2502 \u2022 control_msgs/ \u2502    \u2502 \u2022 Grippers      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   JointCommand  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 \u2022 std_msgs/     \u2502\n                       \u2502   Float64Multi  \u2502\n                       \u2502   Array         \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"real-time-integration-architecture",children:"Real-Time Integration Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    REAL-TIME ARCHITECTURE                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  HIGH FREQUENCY (1000Hz)    \u2502  MEDIUM FREQUENCY (100Hz)             \u2502\n\u2502                             \u2502                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Joint State Control   \u2502\u2502  \u2502 \u2022 Path Planning                  \u2502 \u2502\n\u2502  \u2502 \u2022 Balance Control       \u2502\u2502  \u2502 \u2022 Trajectory Generation          \u2502 \u2502\n\u2502  \u2502 \u2022 Low-Level Motor Ctrl  \u2502\u2502  \u2502 \u2022 State Estimation               \u2502 \u2502\n\u2502  \u2502 \u2022 IMU Processing        \u2502\u2502  \u2502 \u2022 Obstacle Detection             \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2502 \u2022 Task Planning                    \u2502 \u2502\n\u2502                             \u2502  \u2502 \u2022 Behavior Selection             \u2502 \u2502\n\u2502                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  LOW FREQUENCY (10Hz)       \u2502  HIGH-PRIORITY TASKS (1000Hz)        \u2502\n\u2502                             \u2502                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Map Building          \u2502\u2502  \u2502 \u2022 Safety Monitoring              \u2502 \u2502\n\u2502  \u2502 \u2022 Route Planning        \u2502\u2502  \u2502 \u2022 Emergency Stop                 \u2502 \u2502\n\u2502  \u2502 \u2022 High-Level Decision   \u2502\u2502  \u2502 \u2022 Collision Avoidance            \u2502 \u2502\n\u2502  \u2502 \u2022 GUI Updates           \u2502\u2502  \u2502 \u2022 Watchdog                       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"hardware-software-integration-patterns",children:"Hardware-Software Integration Patterns"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              HARDWARE-TO-SOFTWARE INTEGRATION PATTERNS            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  SENSOR \u2192 DRIVER \u2192 ROS 2 MESSAGE \u2192 ALGORITHM \u2192 ACTION             \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  CAM    \u2502\u2500\u2500\u2500\u25b6\u2502  DRIVER \u2502\u2500\u2500\u2500\u25b6\u2502 sensor_msgs/    \u2502\u2500\u2500\u2500\u25b6\u2502  VISION \u2502 \u2502\n\u2502  \u2502         \u2502    \u2502         \u2502    \u2502   Image         \u2502    \u2502  ALGO   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502         \u2502                           \u2502                     \u2502       \u2502\n\u2502         \u2502                           \u25bc                     \u25bc       \u2502\n\u2502         \u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   ROS 2 TOPIC   \u2502\u2500\u2500\u2500\u25b6\u2502  PROCESS  \u2502\u2502\n\u2502                             \u2502   /camera/image \u2502    \u2502   RESULT  \u2502\u2502\n\u2502                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                                                                     \u2502\n\u2502  ACTUATOR \u2190 DRIVER \u2190 ROS 2 MESSAGE \u2190 ALGORITHM \u2190 DECISION          \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 SERVO   \u2502\u25c0\u2500\u2500\u2500\u2502  DRIVER \u2502\u25c0\u2500\u2500\u2500\u2502 std_msgs/       \u2502\u25c0\u2500\u2500\u2500\u2502 CONTROL \u2502 \u2502\n\u2502  \u2502 MOTOR   \u2502    \u2502         \u2502    \u2502   Float64       \u2502    \u2502 ALGO    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502         \u25b2                           \u25b2                     \u25b2       \u2502\n\u2502         \u2502                           \u2502                     \u2502       \u2502\n\u2502         \u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502       \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502   ROS 2 TOPIC   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                             \u2502   /motor/command\u2502                   \u2502\n\u2502                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"communication-protocol-mapping",children:"Communication Protocol Mapping"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                COMMUNICATION PROTOCOL MAPPING                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  HARDWARE INTERFACE LAYER                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Ethernet (1G/10G): High-bandwidth sensors (cameras, LiDAR)  \u2502 \u2502\n\u2502  \u2502 \u2022 CAN Bus: Robust actuator communication                      \u2502 \u2502\n\u2502  \u2502 \u2022 SPI/I2C: High-speed sensor interfaces                       \u2502 \u2502\n\u2502  \u2502 \u2022 USB: Plug-and-play peripheral devices                       \u2502 \u2502\n\u2502  \u2502 \u2022 WiFi/5G: Remote communication and updates                   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                     \u2502\n\u2502  DEVICE DRIVER LAYER                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 ros2_control hardware interface                             \u2502 \u2502\n\u2502  \u2502 \u2022 Sensor-specific drivers (camera, IMU, etc.)                 \u2502 \u2502\n\u2502  \u2502 \u2022 Actuator control interfaces                                   \u2502 \u2502\n\u2502  \u2502 \u2022 Communication protocol abstraction                            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                     \u2502\n\u2502  ROS 2 COMMUNICATION LAYER                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Topics: High-frequency streaming (sensors, commands)        \u2502 \u2502\n\u2502  \u2502 \u2022 Services: Request-response (calibration, configuration)     \u2502 \u2502\n\u2502  \u2502 \u2022 Actions: Goal-oriented (navigation, manipulation)           \u2502 \u2502\n\u2502  \u2502 \u2022 Parameters: Configuration management                          \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                     \u2502\n\u2502  APPLICATION ALGORITHM LAYER                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Perception: Object detection, SLAM, localization            \u2502 \u2502\n\u2502  \u2502 \u2022 Planning: Path planning, motion planning, task planning     \u2502 \u2502\n\u2502  \u2502 \u2022 Control: Joint control, balance control, impedance control  \u2502 \u2502\n\u2502  \u2502 \u2022 Coordination: Multi-robot coordination, human-robot interaction\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture-integration-pattern",children:"System Architecture Integration Pattern"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    SYSTEM ARCHITECTURE INTEGRATION                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                     ROBOT PLATFORM                              \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502 \u2502\n\u2502  \u2502  \u2502   SENSORS   \u2502  \u2502  COMPUTING  \u2502  \u2502 ACTUATORS   \u2502            \u2502 \u2502\n\u2502  \u2502  \u2502             \u2502  \u2502             \u2502  \u2502             \u2502            \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Cameras   \u2502  \u2502 \u2022 Jetson    \u2502  \u2502 \u2022 Servos    \u2502            \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 LiDAR     \u2502\u2500\u2500\u2502 \u2022 RTX GPU   \u2502\u2500\u2500\u2502 \u2022 Motors    \u2502            \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 IMU       \u2502  \u2502 \u2022 CPU       \u2502  \u2502 \u2022 Pneumatics\u2502            \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Force/Tq  \u2502  \u2502 \u2022 RAM       \u2502  \u2502 \u2022 Hydraulics\u2502            \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                    SOFTWARE LAYER                               \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502 \u2502\n\u2502  \u2502  \u2502   PERCEPTION    \u2502 \u2502    PLANNING     \u2502 \u2502    CONTROL      \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502                 \u2502 \u2502                 \u2502 \u2502                 \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Vision        \u2502 \u2502 \u2022 Path Plan     \u2502 \u2502 \u2022 Motion Ctrl   \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 SLAM          \u2502 \u2502 \u2022 Task Plan     \u2502 \u2502 \u2022 Balance Ctrl  \u2502   \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Tracking      \u2502 \u2502 \u2022 Behavior      \u2502 \u2502 \u2022 Safety Sys    \u2502   \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 \u2502\n\u2502  \u2502                                                               \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502                   ROS 2 FRAMEWORK                         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   NODES     \u2502 \u2502   TOPICS    \u2502 \u2502  SERVICES   \u2502         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502             \u2502 \u2502             \u2502 \u2502             \u2502         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Cam Proc  \u2502 \u2502 \u2022 /image    \u2502 \u2502 \u2022 /move     \u2502         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Nav Stack \u2502 \u2502 \u2022 /scan     \u2502 \u2502 \u2022 /servo    \u2502         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Control   \u2502 \u2502 \u2022 /cmd_vel  \u2502 \u2502 \u2022 /gripper  \u2502         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"hardware-software-integration-exercises",children:"Hardware-Software Integration Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-camera-sensor-integration",children:"Exercise 1: Camera-Sensor Integration"}),"\n",(0,t.jsx)(n.h4,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Connect a camera sensor to the perception pipeline and implement basic image processing."}),"\n",(0,t.jsx)(n.h4,{id:"scenario",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"You have a RGB camera mounted on the humanoid robot's head. Your task is to create a ROS 2 node that subscribes to the camera feed and performs basic object detection."}),"\n",(0,t.jsx)(n.h4,{id:"steps",children:"Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Hardware Setup"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Verify camera is detected\nls /dev/video*\n\n# Check camera capabilities\nv4l2-ctl --device=/dev/video0 --list-formats-ext\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Software Implementation"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# camera_perception_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass CameraPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'camera_perception_node\')\n\n        # Create subscriber for camera image\n        self.subscription = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Create publisher for processed image\n        self.publisher = self.create_publisher(\n            Image,\n            \'/camera/processed/image\',\n            10\n        )\n\n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n\n        self.get_logger().info("Camera perception node initialized")\n\n    def image_callback(self, msg):\n        """Process incoming camera image."""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Perform basic object detection (color-based for simplicity)\n            processed_image = self.detect_red_objects(cv_image)\n\n            # Convert back to ROS image format\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding=\'bgr8\')\n            processed_msg.header = msg.header  # Preserve timestamp and frame ID\n\n            # Publish processed image\n            self.publisher.publish(processed_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def detect_red_objects(self, image):\n        """Detect red objects in image."""\n        # Convert BGR to HSV for better color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color\n        lower_red1 = np.array([0, 50, 50])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n\n        # Create masks for red color\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n        mask = mask1 + mask2\n\n        # Apply morphological operations to clean up mask\n        kernel = np.ones((5, 5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours of red objects\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw bounding boxes around detected objects\n        result = image.copy()\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small objects\n                x, y, w, h = cv2.boundingRect(contour)\n                cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n                # Calculate center of object\n                center_x, center_y = x + w//2, y + h//2\n                cv2.circle(result, (center_x, center_y), 5, (255, 0, 0), -1)\n\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraPerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Launch Configuration"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- camera_integration.launch.xml --\x3e\n<launch>\n    \x3c!-- Launch camera driver --\x3e\n    <node pkg="usb_cam" exec="usb_cam_node_exe" name="camera_driver">\n        <param name="video_device" value="/dev/video0"/>\n        <param name="image_width" value="640"/>\n        <param name="image_height" value="480"/>\n        <param name="framerate" value="30"/>\n        <param name="camera_name" value="head_camera"/>\n    </node>\n\n    \x3c!-- Launch perception node --\x3e\n    <node pkg="your_package" exec="camera_perception_node.py" name="camera_perception">\n        <remap from="/camera/rgb/image_raw" to="/head_camera/image_raw"/>\n    </node>\n</launch>\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Validation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Verify the camera feed is being published"}),"\n",(0,t.jsx)(n.li,{children:"Check that processed images show bounding boxes around red objects"}),"\n",(0,t.jsx)(n.li,{children:"Measure processing latency and throughput"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-imu-integration-for-balance-control",children:"Exercise 2: IMU-Integration for Balance Control"}),"\n",(0,t.jsx)(n.h4,{id:"objective-1",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Integrate IMU data with balance control algorithms to maintain robot stability."}),"\n",(0,t.jsx)(n.h4,{id:"scenario-1",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Your humanoid robot needs to maintain balance when standing. Use IMU data to implement a feedback control system."}),"\n",(0,t.jsx)(n.h4,{id:"steps-1",children:"Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Hardware Connection"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check IMU connection\nls /dev/i2c-*  # or /dev/spi*\n\n# Verify IMU data stream\nros2 topic echo /imu/data_raw\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Software Implementation"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# balance_control_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass BalanceControlNode(Node):\n    def __init__(self):\n        super().__init__(\'balance_control_node\')\n\n        # Subscribe to IMU data\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data_raw\',\n            self.imu_callback,\n            10\n        )\n\n        # Publishers for balance correction commands\n        self.ankle_roll_pub = self.create_publisher(Float64, \'/ankle_roll/command\', 10)\n        self.ankle_pitch_pub = self.create_publisher(Float64, \'/ankle_pitch/command\', 10)\n\n        # PID controllers for balance\n        self.roll_pid = PIDController(kp=2.0, ki=0.1, kd=0.05)\n        self.pitch_pid = PIDController(kp=2.0, ki=0.1, kd=0.05)\n\n        # Target angles (should be 0 for perfect balance)\n        self.target_roll = 0.0\n        self.target_pitch = 0.0\n\n        # Low-pass filter for noisy IMU data\n        self.alpha = 0.1  # Filter coefficient\n        self.filtered_roll = 0.0\n        self.filtered_pitch = 0.0\n\n        self.get_logger().info("Balance control node initialized")\n\n    def imu_callback(self, msg):\n        """Process IMU data for balance control."""\n        # Extract orientation from IMU quaternion\n        quat = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        rotation = R.from_quat(quat)\n        roll, pitch, yaw = rotation.as_euler(\'xyz\')\n\n        # Apply low-pass filter to reduce noise\n        self.filtered_roll = self.alpha * roll + (1 - self.alpha) * self.filtered_roll\n        self.filtered_pitch = self.alpha * pitch + (1 - self.alpha) * self.filtered_pitch\n\n        # Calculate control commands using PID\n        roll_command = self.roll_pid.compute(self.target_roll, self.filtered_roll)\n        pitch_command = self.pitch_pid.compute(self.target_pitch, self.filtered_pitch)\n\n        # Publish balance correction commands\n        roll_msg = Float64()\n        roll_msg.data = float(roll_command)\n        self.ankle_roll_pub.publish(roll_msg)\n\n        pitch_msg = Float64()\n        pitch_msg.data = float(pitch_command)\n        self.ankle_pitch_pub.publish(pitch_msg)\n\n        # Log balance state\n        self.get_logger().debug(f"Roll: {np.degrees(self.filtered_roll):.2f}\xb0, "\n                               f"Pitch: {np.degrees(self.filtered_pitch):.2f}\xb0, "\n                               f"Commands - Roll: {roll_command:.3f}, Pitch: {pitch_command:.3f}")\n\nclass PIDController:\n    def __init__(self, kp=1.0, ki=0.0, kd=0.0, dt=0.01):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.dt = dt\n\n        self.previous_error = 0.0\n        self.integral = 0.0\n\n    def compute(self, target, current):\n        """Compute PID output."""\n        error = target - current\n\n        # Proportional term\n        p_term = self.kp * error\n\n        # Integral term\n        self.integral += error * self.dt\n        i_term = self.ki * self.integral\n\n        # Derivative term\n        derivative = (error - self.previous_error) / self.dt\n        d_term = self.kd * derivative\n\n        # Store current error for next iteration\n        self.previous_error = error\n\n        # Calculate output\n        output = p_term + i_term + d_term\n\n        # Apply saturation limits\n        output = max(min(output, 1.0), -1.0)\n\n        return output\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = BalanceControlNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Validation and Testing"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# balance_validation_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nimport numpy as np\n\nclass BalanceValidator(Node):\n    def __init__(self):\n        super().__init__(\'balance_validator\')\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data_raw\',\n            self.imu_callback,\n            10\n        )\n\n        # Statistics for balance quality\n        self.roll_history = []\n        self.pitch_history = []\n        self.window_size = 100  # Number of samples to analyze\n\n        # Timers for periodic analysis\n        self.analysis_timer = self.create_timer(2.0, self.analyze_balance)\n\n    def imu_callback(self, msg):\n        """Record IMU data for balance analysis."""\n        # Extract orientation\n        from scipy.spatial.transform import Rotation as R\n        quat = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        rotation = R.from_quat(quat)\n        roll, pitch, _ = rotation.as_euler(\'xyz\')\n\n        # Maintain history window\n        self.roll_history.append(np.degrees(roll))\n        self.pitch_history.append(np.degrees(pitch))\n\n        # Keep only recent samples\n        if len(self.roll_history) > self.window_size:\n            self.roll_history.pop(0)\n            self.pitch_history.pop(0)\n\n    def analyze_balance(self):\n        """Analyze balance quality."""\n        if len(self.roll_history) < 10:  # Need minimum samples\n            return\n\n        # Calculate statistics\n        avg_roll = np.mean(self.roll_history)\n        avg_pitch = np.mean(self.pitch_history)\n        std_roll = np.std(self.roll_history)\n        std_pitch = np.std(self.pitch_history)\n\n        # Balance quality metrics\n        stability_score = 1.0 / (1.0 + std_roll + std_pitch)  # Higher is better\n        tilt_score = 1.0 / (1.0 + abs(avg_roll) + abs(avg_pitch))  # Higher is better\n\n        self.get_logger().info(f"Balance Analysis:")\n        self.get_logger().info(f"  Average Tilt - Roll: {avg_roll:.2f}\xb0, Pitch: {avg_pitch:.2f}\xb0")\n        self.get_logger().info(f"  Stability (std dev) - Roll: {std_roll:.2f}\xb0, Pitch: {std_pitch:.2f}\xb0")\n        self.get_logger().info(f"  Stability Score: {stability_score:.3f}")\n        self.get_logger().info(f"  Tilt Score: {tilt_score:.3f}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = BalanceValidator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-multi-sensor-fusion-exercise",children:"Exercise 3: Multi-Sensor Fusion Exercise"}),"\n",(0,t.jsx)(n.h4,{id:"objective-2",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Combine data from multiple sensors (camera, LiDAR, IMU) to create a more robust perception system."}),"\n",(0,t.jsx)(n.h4,{id:"scenario-2",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Implement a sensor fusion system that combines visual, depth, and inertial data to track objects in 3D space."}),"\n",(0,t.jsx)(n.h4,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# sensor_fusion_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu\nfrom geometry_msgs.msg import PointStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport cv2\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Subscribers for all sensor data\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10\n        )\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/scan', self.lidar_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data_raw', self.imu_callback, 10\n        )\n\n        # Publisher for fused object positions\n        self.object_pub = self.create_publisher(PointStamped, '/fused_objects/positions', 10)\n\n        # Initialize data storage\n        self.latest_camera_data = None\n        self.latest_lidar_data = None\n        self.latest_imu_data = None\n\n        # Coordinate transformation matrices\n        self.cam_to_robot = self.get_camera_to_robot_transform()  # From calibration\n        self.lidar_to_robot = self.get_lidar_to_robot_transform()  # From calibration\n\n        # Object tracking variables\n        self.tracked_objects = {}\n        self.next_object_id = 0\n\n        # OpenCV bridge\n        self.bridge = CvBridge()\n\n        self.get_logger().info(\"Sensor fusion node initialized\")\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera data for object detection.\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Detect objects in image\n            image_objects = self.detect_objects_in_image(cv_image)\n\n            # Store with timestamp for synchronization\n            self.latest_camera_data = {\n                'image': cv_image,\n                'objects': image_objects,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n\n            # Attempt fusion if other sensor data is available\n            self.attempt_sensor_fusion()\n\n        except Exception as e:\n            self.get_logger().error(f\"Camera callback error: {e}\")\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LiDAR data for object detection.\"\"\"\n        try:\n            # Convert scan to point cloud\n            points = self.scan_to_cartesian(msg)\n\n            # Detect objects in point cloud\n            lidar_objects = self.detect_objects_in_pointcloud(points)\n\n            # Store with timestamp\n            self.latest_lidar_data = {\n                'points': points,\n                'objects': lidar_objects,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n\n            # Attempt fusion if other sensor data is available\n            self.attempt_sensor_fusion()\n\n        except Exception as e:\n            self.get_logger().error(f\"LiDAR callback error: {e}\")\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data for orientation.\"\"\"\n        self.latest_imu_data = {\n            'orientation': msg.orientation,\n            'angular_velocity': msg.angular_velocity,\n            'linear_acceleration': msg.linear_acceleration,\n            'timestamp': msg.header.stamp\n        }\n\n    def detect_objects_in_image(self, image):\n        \"\"\"Detect objects in camera image.\"\"\"\n        # Simple color-based detection for demonstration\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Detect red objects\n        lower_red1 = np.array([0, 50, 50])\n        upper_red1 = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n\n        mask = mask1 + mask2\n\n        # Apply morphological operations\n        kernel = np.ones((5, 5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Minimum size filter\n                x, y, w, h = cv2.boundingRect(contour)\n\n                # Calculate image coordinates\n                center_x = x + w // 2\n                center_y = y + h // 2\n\n                objects.append({\n                    'type': 'red_object',\n                    'bbox': (x, y, w, h),\n                    'center': (center_x, center_y),\n                    'area': cv2.contourArea(contour)\n                })\n\n        return objects\n\n    def scan_to_cartesian(self, scan_msg):\n        \"\"\"Convert laser scan to Cartesian coordinates.\"\"\"\n        points = []\n        angle = scan_msg.angle_min\n\n        for r in scan_msg.ranges:\n            if not (np.isnan(r) or np.isinf(r)) and scan_msg.range_min <= r <= scan_msg.range_max:\n                x = r * np.cos(angle)\n                y = r * np.sin(angle)\n                points.append([x, y])\n            angle += scan_msg.angle_increment\n\n        return np.array(points)\n\n    def detect_objects_in_pointcloud(self, points):\n        \"\"\"Detect objects in 2D point cloud.\"\"\"\n        if len(points) < 10:\n            return []\n\n        # Simple clustering for object detection\n        from sklearn.cluster import DBSCAN\n\n        clustering = DBSCAN(eps=0.3, min_samples=10).fit(points)\n        labels = clustering.labels_\n\n        objects = []\n        for label in set(labels):\n            if label == -1:  # Noise points\n                continue\n\n            # Get points belonging to this cluster\n            cluster_points = points[labels == label]\n\n            # Calculate cluster center\n            center = np.mean(cluster_points, axis=0)\n\n            objects.append({\n                'type': 'clustered_object',\n                'center': center,\n                'points': cluster_points,\n                'size': len(cluster_points)\n            })\n\n        return objects\n\n    def attempt_sensor_fusion(self):\n        \"\"\"Attempt to fuse sensor data if all required data is available.\"\"\"\n        if not all([self.latest_camera_data, self.latest_lidar_data]):\n            return  # Not enough data yet\n\n        # Check temporal synchronization (within 100ms)\n        cam_time = self.latest_camera_data['timestamp']\n        lidar_time = self.latest_lidar_data['timestamp']\n\n        time_diff = abs(cam_time.sec + cam_time.nanosec * 1e-9 -\n                       lidar_time.sec - lidar_time.nanosec * 1e-9)\n\n        if time_diff > 0.1:  # More than 100ms apart\n            return\n\n        # Perform fusion\n        fused_objects = self.fuse_camera_lidar_data(\n            self.latest_camera_data,\n            self.latest_lidar_data\n        )\n\n        # Publish fused objects\n        for obj in fused_objects:\n            point_msg = PointStamped()\n            point_msg.header.stamp = self.get_clock().now().to_msg()\n            point_msg.header.frame_id = \"robot_base_frame\"\n            point_msg.point.x = obj['world_position'][0]\n            point_msg.point.y = obj['world_position'][1]\n            point_msg.point.z = obj['world_position'][2]\n\n            self.object_pub.publish(point_msg)\n\n    def fuse_camera_lidar_data(self, camera_data, lidar_data):\n        \"\"\"Fuse camera and LiDAR object detections.\"\"\"\n        fused_objects = []\n\n        # For each camera object, try to find corresponding LiDAR object\n        for cam_obj in camera_data['objects']:\n            # Project image coordinates to 3D ray\n            img_x, img_y = cam_obj['center']\n\n            # Convert image coordinates to angles (simplified pinhole model)\n            # In real implementation, use calibrated camera parameters\n            fov_x = 60 * np.pi / 180  # 60 degrees in radians\n            fov_y = 45 * np.pi / 180  # 45 degrees in radians\n\n            img_width = camera_data['image'].shape[1]\n            img_height = camera_data['image'].shape[0]\n\n            angle_x = (img_x / img_width - 0.5) * fov_x\n            angle_y = (img_y / img_height - 0.5) * fov_y\n\n            # Look for LiDAR points in the direction of this object\n            for lidar_obj in lidar_data['objects']:\n                lidar_x, lidar_y = lidar_obj['center']\n\n                # Calculate distance in image space approximation\n                # In real implementation, use proper camera-LiDAR calibration\n                distance_estimate = np.sqrt(lidar_x**2 + lidar_y**2)\n\n                # Calculate expected image position for this LiDAR point\n                expected_x = (np.arctan2(lidar_x, distance_estimate) / fov_x + 0.5) * img_width\n                expected_y = (np.arctan2(lidar_y, distance_estimate) / fov_y + 0.5) * img_height\n\n                # Check if they match (within threshold)\n                position_diff = np.sqrt((img_x - expected_x)**2 + (img_y - expected_y)**2)\n\n                if position_diff < 50:  # 50 pixel threshold\n                    # Create fused object with combined information\n                    fused_object = {\n                        'type': f\"fused_{cam_obj['type']}_{lidar_obj['type']}\",\n                        'camera_info': cam_obj,\n                        'lidar_info': lidar_obj,\n                        'world_position': [lidar_x, lidar_y, 0.0],  # Z=0 for ground plane\n                        'confidence': 0.8  # High confidence for matched objects\n                    }\n\n                    fused_objects.append(fused_object)\n\n        return fused_objects\n\n    def get_camera_to_robot_transform(self):\n        \"\"\"Get calibrated transform from camera to robot base.\"\"\"\n        # In real implementation, load from calibration file\n        # This is a placeholder identity transform\n        return np.eye(4)\n\n    def get_lidar_to_robot_transform(self):\n        \"\"\"Get calibrated transform from LiDAR to robot base.\"\"\"\n        # In real implementation, load from calibration file\n        # This is a placeholder identity transform\n        return np.eye(4)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorFusionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    REAL-TIME ARCHITECTURE                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  HIGH FREQUENCY (1000Hz)    \u2502  MEDIUM FREQUENCY (100Hz)             \u2502\n\u2502                             \u2502                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Joint State Control   \u2502\u2502  \u2502 \u2022 Path Planning                  \u2502 \u2502\n\u2502  \u2502 \u2022 Balance Control       \u2502\u2502  \u2502 \u2022 Trajectory Generation          \u2502 \u2502\n\u2502  \u2502 \u2022 Low-Level Motor Ctrl  \u2502\u2502  \u2502 \u2022 State Estimation               \u2502 \u2502\n\u2502  \u2502 \u2022 IMU Processing        \u2502\u2502  \u2502 \u2022 Obstacle Detection             \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2502 \u2022 Task Planning                    \u2502 \u2502\n\u2502                             \u2502  \u2502 \u2022 Behavior Selection             \u2502 \u2502\n\u2502                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  LOW FREQUENCY (10Hz)       \u2502  HIGH-PRIORITY TASKS (1000Hz)        \u2502\n\u2502                             \u2502                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \u2022 Map Building          \u2502\u2502  \u2502 \u2022 Safety Monitoring              \u2502 \u2502\n\u2502  \u2502 \u2022 Route Planning        \u2502\u2502  \u2502 \u2022 Emergency Stop                 \u2502 \u2502\n\u2502  \u2502 \u2022 High-Level Decision   \u2502\u2502  \u2502 \u2022 Collision Avoidance            \u2502 \u2502\n\u2502  \u2502 \u2022 GUI Updates           \u2502\u2502  \u2502 \u2022 Watchdog                       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"hardware-software-integration-exercises-1",children:"Hardware-Software Integration Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-camera-sensor-integration-1",children:"Exercise 1: Camera-Sensor Integration"}),"\n",(0,t.jsx)(n.h4,{id:"objective-3",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Connect a camera sensor to the perception pipeline and implement basic image processing."}),"\n",(0,t.jsx)(n.h4,{id:"scenario-3",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"You have a RGB camera mounted on the humanoid robot's head. Your task is to create a ROS 2 node that subscribes to the camera feed and performs basic object detection."}),"\n",(0,t.jsx)(n.h4,{id:"steps-2",children:"Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Hardware Setup"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Verify camera is detected\nls /dev/video*\n\n# Check camera capabilities\nv4l2-ctl --device=/dev/video0 --list-formats-ext\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Software Implementation"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# camera_perception_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass CameraPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'camera_perception_node\')\n\n        # Create subscriber for camera image\n        self.subscription = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Create publisher for processed image\n        self.publisher = self.create_publisher(\n            Image,\n            \'/camera/processed/image\',\n            10\n        )\n\n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n\n        self.get_logger().info("Camera perception node initialized")\n\n    def image_callback(self, msg):\n        """Process incoming camera image."""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Perform basic object detection (color-based for simplicity)\n            processed_image = self.detect_red_objects(cv_image)\n\n            # Convert back to ROS image format\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding=\'bgr8\')\n            processed_msg.header = msg.header  # Preserve timestamp and frame ID\n\n            # Publish processed image\n            self.publisher.publish(processed_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def detect_red_objects(self, image):\n        """Detect red objects in image."""\n        # Convert BGR to HSV for better color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color\n        lower_red1 = np.array([0, 50, 50])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n\n        # Create masks for red color\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n        mask = mask1 + mask2\n\n        # Apply morphological operations to clean up mask\n        kernel = np.ones((5, 5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours of red objects\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw bounding boxes around detected objects\n        result = image.copy()\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small objects\n                x, y, w, h = cv2.boundingRect(contour)\n                cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n                # Calculate center of object\n                center_x, center_y = x + w//2, y + h//2\n                cv2.circle(result, (center_x, center_y), 5, (255, 0, 0), -1)\n\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraPerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Launch Configuration"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- camera_integration.launch.xml --\x3e\n<launch>\n    \x3c!-- Launch camera driver --\x3e\n    <node pkg="usb_cam" exec="usb_cam_node_exe" name="camera_driver">\n        <param name="video_device" value="/dev/video0"/>\n        <param name="image_width" value="640"/>\n        <param name="image_height" value="480"/>\n        <param name="framerate" value="30"/>\n        <param name="camera_name" value="head_camera"/>\n    </node>\n\n    \x3c!-- Launch perception node --\x3e\n    <node pkg="your_package" exec="camera_perception_node.py" name="camera_perception">\n        <remap from="/camera/rgb/image_raw" to="/head_camera/image_raw"/>\n    </node>\n</launch>\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Validation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Verify the camera feed is being published"}),"\n",(0,t.jsx)(n.li,{children:"Check that processed images show bounding boxes around red objects"}),"\n",(0,t.jsx)(n.li,{children:"Measure processing latency and throughput"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-imu-integration-for-balance-control-1",children:"Exercise 2: IMU-Integration for Balance Control"}),"\n",(0,t.jsx)(n.h4,{id:"objective-4",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Integrate IMU data with balance control algorithms to maintain robot stability."}),"\n",(0,t.jsx)(n.h4,{id:"scenario-4",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Your humanoid robot needs to maintain balance when standing. Use IMU data to implement a feedback control system."}),"\n",(0,t.jsx)(n.h4,{id:"steps-3",children:"Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Hardware Connection"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check IMU connection\nls /dev/i2c-*  # or /dev/spi*\n\n# Verify IMU data stream\nros2 topic echo /imu/data_raw\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Software Implementation"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# balance_control_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass BalanceControlNode(Node):\n    def __init__(self):\n        super().__init__(\'balance_control_node\')\n\n        # Subscribe to IMU data\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data_raw\',\n            self.imu_callback,\n            10\n        )\n\n        # Publishers for balance correction commands\n        self.ankle_roll_pub = self.create_publisher(Float64, \'/ankle_roll/command\', 10)\n        self.ankle_pitch_pub = self.create_publisher(Float64, \'/ankle_pitch/command\', 10)\n\n        # PID controllers for balance\n        self.roll_pid = PIDController(kp=2.0, ki=0.1, kd=0.05)\n        self.pitch_pid = PIDController(kp=2.0, ki=0.1, kd=0.05)\n\n        # Target angles (should be 0 for perfect balance)\n        self.target_roll = 0.0\n        self.target_pitch = 0.0\n\n        # Low-pass filter for noisy IMU data\n        self.alpha = 0.1  # Filter coefficient\n        self.filtered_roll = 0.0\n        self.filtered_pitch = 0.0\n\n        self.get_logger().info("Balance control node initialized")\n\n    def imu_callback(self, msg):\n        """Process IMU data for balance control."""\n        # Extract orientation from IMU quaternion\n        quat = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        rotation = R.from_quat(quat)\n        roll, pitch, yaw = rotation.as_euler(\'xyz\')\n\n        # Apply low-pass filter to reduce noise\n        self.filtered_roll = self.alpha * roll + (1 - self.alpha) * self.filtered_roll\n        self.filtered_pitch = self.alpha * pitch + (1 - self.alpha) * self.filtered_pitch\n\n        # Calculate control commands using PID\n        roll_command = self.roll_pid.compute(self.target_roll, self.filtered_roll)\n        pitch_command = self.pitch_pid.compute(self.target_pitch, self.filtered_pitch)\n\n        # Publish balance correction commands\n        roll_msg = Float64()\n        roll_msg.data = float(roll_command)\n        self.ankle_roll_pub.publish(roll_msg)\n\n        pitch_msg = Float64()\n        pitch_msg.data = float(pitch_command)\n        self.ankle_pitch_pub.publish(pitch_msg)\n\n        # Log balance state\n        self.get_logger().debug(f"Roll: {np.degrees(self.filtered_roll):.2f}\xb0, "\n                               f"Pitch: {np.degrees(self.filtered_pitch):.2f}\xb0, "\n                               f"Commands - Roll: {roll_command:.3f}, Pitch: {pitch_command:.3f}")\n\nclass PIDController:\n    def __init__(self, kp=1.0, ki=0.0, kd=0.0, dt=0.01):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.dt = dt\n\n        self.previous_error = 0.0\n        self.integral = 0.0\n\n    def compute(self, target, current):\n        """Compute PID output."""\n        error = target - current\n\n        # Proportional term\n        p_term = self.kp * error\n\n        # Integral term\n        self.integral += error * self.dt\n        i_term = self.ki * self.integral\n\n        # Derivative term\n        derivative = (error - self.previous_error) / self.dt\n        d_term = self.kd * derivative\n\n        # Store current error for next iteration\n        self.previous_error = error\n\n        # Calculate output\n        output = p_term + i_term + d_term\n\n        # Apply saturation limits\n        output = max(min(output, 1.0), -1.0)\n\n        return output\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = BalanceControlNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Validation and Testing"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# balance_validation_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nimport numpy as np\n\nclass BalanceValidator(Node):\n    def __init__(self):\n        super().__init__(\'balance_validator\')\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data_raw\',\n            self.imu_callback,\n            10\n        )\n\n        # Statistics for balance quality\n        self.roll_history = []\n        self.pitch_history = []\n        self.window_size = 100  # Number of samples to analyze\n\n        # Timers for periodic analysis\n        self.analysis_timer = self.create_timer(2.0, self.analyze_balance)\n\n    def imu_callback(self, msg):\n        """Record IMU data for balance analysis."""\n        # Extract orientation\n        from scipy.spatial.transform import Rotation as R\n        quat = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        rotation = R.from_quat(quat)\n        roll, pitch, _ = rotation.as_euler(\'xyz\')\n\n        # Maintain history window\n        self.roll_history.append(np.degrees(roll))\n        self.pitch_history.append(np.degrees(pitch))\n\n        # Keep only recent samples\n        if len(self.roll_history) > self.window_size:\n            self.roll_history.pop(0)\n            self.pitch_history.pop(0)\n\n    def analyze_balance(self):\n        """Analyze balance quality."""\n        if len(self.roll_history) < 10:  # Need minimum samples\n            return\n\n        # Calculate statistics\n        avg_roll = np.mean(self.roll_history)\n        avg_pitch = np.mean(self.pitch_history)\n        std_roll = np.std(self.roll_history)\n        std_pitch = np.std(self.pitch_history)\n\n        # Balance quality metrics\n        stability_score = 1.0 / (1.0 + std_roll + std_pitch)  # Higher is better\n        tilt_score = 1.0 / (1.0 + abs(avg_roll) + abs(avg_pitch))  # Higher is better\n\n        self.get_logger().info(f"Balance Analysis:")\n        self.get_logger().info(f"  Average Tilt - Roll: {avg_roll:.2f}\xb0, Pitch: {avg_pitch:.2f}\xb0")\n        self.get_logger().info(f"  Stability (std dev) - Roll: {std_roll:.2f}\xb0, Pitch: {std_pitch:.2f}\xb0")\n        self.get_logger().info(f"  Stability Score: {stability_score:.3f}")\n        self.get_logger().info(f"  Tilt Score: {tilt_score:.3f}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = BalanceValidator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-multi-sensor-fusion-exercise-1",children:"Exercise 3: Multi-Sensor Fusion Exercise"}),"\n",(0,t.jsx)(n.h4,{id:"objective-5",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Combine data from multiple sensors (camera, LiDAR, IMU) to create a more robust perception system."}),"\n",(0,t.jsx)(n.h4,{id:"scenario-5",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Implement a sensor fusion system that combines visual, depth, and inertial data to track objects in 3D space."}),"\n",(0,t.jsx)(n.h4,{id:"implementation-1",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# sensor_fusion_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu\nfrom geometry_msgs.msg import PointStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport cv2\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Subscribers for all sensor data\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10\n        )\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/scan', self.lidar_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data_raw', self.imu_callback, 10\n        )\n\n        # Publisher for fused object positions\n        self.object_pub = self.create_publisher(PointStamped, '/fused_objects/positions', 10)\n\n        # Initialize data storage\n        self.latest_camera_data = None\n        self.latest_lidar_data = None\n        self.latest_imu_data = None\n\n        # Coordinate transformation matrices\n        self.cam_to_robot = self.get_camera_to_robot_transform()  # From calibration\n        self.lidar_to_robot = self.get_lidar_to_robot_transform()  # From calibration\n\n        # Object tracking variables\n        self.tracked_objects = {}\n        self.next_object_id = 0\n\n        # OpenCV bridge\n        self.bridge = CvBridge()\n\n        self.get_logger().info(\"Sensor fusion node initialized\")\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera data for object detection.\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Detect objects in image\n            image_objects = self.detect_objects_in_image(cv_image)\n\n            # Store with timestamp for synchronization\n            self.latest_camera_data = {\n                'image': cv_image,\n                'objects': image_objects,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n\n            # Attempt fusion if other sensor data is available\n            self.attempt_sensor_fusion()\n\n        except Exception as e:\n            self.get_logger().error(f\"Camera callback error: {e}\")\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LiDAR data for object detection.\"\"\"\n        try:\n            # Convert scan to point cloud\n            points = self.scan_to_cartesian(msg)\n\n            # Detect objects in point cloud\n            lidar_objects = self.detect_objects_in_pointcloud(points)\n\n            # Store with timestamp\n            self.latest_lidar_data = {\n                'points': points,\n                'objects': lidar_objects,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n\n            # Attempt fusion if other sensor data is available\n            self.attempt_sensor_fusion()\n\n        except Exception as e:\n            self.get_logger().error(f\"LiDAR callback error: {e}\")\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data for orientation.\"\"\"\n        self.latest_imu_data = {\n            'orientation': msg.orientation,\n            'angular_velocity': msg.angular_velocity,\n            'linear_acceleration': msg.linear_acceleration,\n            'timestamp': msg.header.stamp\n        }\n\n    def detect_objects_in_image(self, image):\n        \"\"\"Detect objects in camera image.\"\"\"\n        # Simple color-based detection for demonstration\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Detect red objects\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n\n        # Apply morphological operations\n        kernel = np.ones((5, 5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Minimum size filter\n                x, y, w, h = cv2.boundingRect(contour)\n\n                # Calculate image coordinates\n                center_x = x + w // 2\n                center_y = y + h // 2\n\n                objects.append({\n                    'type': 'red_object',\n                    'bbox': (x, y, w, h),\n                    'center': (center_x, center_y),\n                    'area': cv2.contourArea(contour)\n                })\n\n        return objects\n\n    def scan_to_cartesian(self, scan_msg):\n        \"\"\"Convert laser scan to Cartesian coordinates.\"\"\"\n        points = []\n        angle = scan_msg.angle_min\n\n        for r in scan_msg.ranges:\n            if not (np.isnan(r) or np.isinf(r)) and scan_msg.range_min <= r <= scan_msg.range_max:\n                x = r * np.cos(angle)\n                y = r * np.sin(angle)\n                points.append([x, y])\n            angle += scan_msg.angle_increment\n\n        return np.array(points)\n\n    def detect_objects_in_pointcloud(self, points):\n        \"\"\"Detect objects in 2D point cloud.\"\"\"\n        if len(points) < 10:\n            return []\n\n        # Simple clustering for object detection\n        from sklearn.cluster import DBSCAN\n\n        clustering = DBSCAN(eps=0.3, min_samples=10).fit(points)\n        labels = clustering.labels_\n\n        objects = []\n        for label in set(labels):\n            if label == -1:  # Noise points\n                continue\n\n            # Get points belonging to this cluster\n            cluster_points = points[labels == label]\n\n            # Calculate cluster center\n            center = np.mean(cluster_points, axis=0)\n\n            objects.append({\n                'type': 'clustered_object',\n                'center': center,\n                'points': cluster_points,\n                'size': len(cluster_points)\n            })\n\n        return objects\n\n    def attempt_sensor_fusion(self):\n        \"\"\"Attempt to fuse sensor data if all required data is available.\"\"\"\n        if not all([self.latest_camera_data, self.latest_lidar_data]):\n            return  # Not enough data yet\n\n        # Check temporal synchronization (within 100ms)\n        cam_time = self.latest_camera_data['timestamp']\n        lidar_time = self.latest_lidar_data['timestamp']\n\n        time_diff = abs(cam_time.sec + cam_time.nanosec * 1e-9 -\n                       lidar_time.sec - lidar_time.nanosec * 1e-9)\n\n        if time_diff > 0.1:  # More than 100ms apart\n            return\n\n        # Perform fusion\n        fused_objects = self.fuse_camera_lidar_data(\n            self.latest_camera_data,\n            self.latest_lidar_data\n        )\n\n        # Publish fused objects\n        for obj in fused_objects:\n            point_msg = PointStamped()\n            point_msg.header.stamp = self.get_clock().now().to_msg()\n            point_msg.header.frame_id = \"robot_base_frame\"\n            point_msg.point.x = obj['world_position'][0]\n            point_msg.point.y = obj['world_position'][1]\n            point_msg.point.z = obj['world_position'][2]\n\n            self.object_pub.publish(point_msg)\n\n    def fuse_camera_lidar_data(self, camera_data, lidar_data):\n        \"\"\"Fuse camera and LiDAR object detections.\"\"\"\n        fused_objects = []\n\n        # For each camera object, try to find corresponding LiDAR object\n        for cam_obj in camera_data['objects']:\n            # Project image coordinates to 3D ray\n            img_x, img_y = cam_obj['center']\n\n            # Convert image coordinates to angles (simplified pinhole model)\n            # In real implementation, use calibrated camera parameters\n            fov_x = 60 * np.pi / 180  # 60 degrees in radians\n            fov_y = 45 * np.pi / 180  # 45 degrees in radians\n\n            img_width = camera_data['image'].shape[1]\n            img_height = camera_data['image'].shape[0]\n\n            angle_x = (img_x / img_width - 0.5) * fov_x\n            angle_y = (img_y / img_height - 0.5) * fov_y\n\n            # Look for LiDAR points in the direction of this object\n            for lidar_obj in lidar_data['objects']:\n                lidar_x, lidar_y = lidar_obj['center']\n\n                # Calculate distance in image space approximation\n                # In real implementation, use proper camera-LiDAR calibration\n                distance_estimate = np.sqrt(lidar_x**2 + lidar_y**2)\n\n                # Calculate expected image position for this LiDAR point\n                expected_x = (np.arctan2(lidar_x, distance_estimate) / fov_x + 0.5) * img_width\n                expected_y = (np.arctan2(lidar_y, distance_estimate) / fov_y + 0.5) * img_height\n\n                # Check if they match (within threshold)\n                position_diff = np.sqrt((img_x - expected_x)**2 + (img_y - expected_y)**2)\n\n                if position_diff < 50:  # 50 pixel threshold\n                    # Create fused object with combined information\n                    fused_object = {\n                        'type': f\"fused_{cam_obj['type']}_{lidar_obj['type']}\",\n                        'camera_info': cam_obj,\n                        'lidar_info': lidar_obj,\n                        'world_position': [lidar_x, lidar_y, 0.0],  # Z=0 for ground plane\n                        'confidence': 0.8  # High confidence for matched objects\n                    }\n\n                    fused_objects.append(fused_object)\n\n        return fused_objects\n\n    def get_camera_to_robot_transform(self):\n        \"\"\"Get calibrated transform from camera to robot base.\"\"\"\n        # In real implementation, load from calibration file\n        # This is a placeholder identity transform\n        return np.eye(4)\n\n    def get_lidar_to_robot_transform(self):\n        \"\"\"Get calibrated transform from LiDAR to robot base.\"\"\"\n        # In real implementation, load from calibration file\n        # This is a placeholder identity transform\n        return np.eye(4)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorFusionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-best-practices",children:"Integration Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"1-data-synchronization",children:"1. Data Synchronization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use message filters for time synchronization"}),"\n",(0,t.jsx)(n.li,{children:"Implement buffering for asynchronous sensors"}),"\n",(0,t.jsx)(n.li,{children:"Consider interpolation for different frequencies"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-calibration",children:"2. Calibration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Perform regular extrinsic calibration"}),"\n",(0,t.jsx)(n.li,{children:"Validate calibration accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Monitor for calibration drift"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-fault-tolerance",children:"3. Fault Tolerance"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement sensor health monitoring"}),"\n",(0,t.jsx)(n.li,{children:"Provide fallback behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Graceful degradation strategies"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-real-time-considerations",children:"4. Real-Time Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Prioritize critical control loops"}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate threading models"}),"\n",(0,t.jsx)(n.li,{children:"Monitor computational load"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-integration-issues",children:"Troubleshooting Integration Issues"}),"\n",(0,t.jsx)(n.h3,{id:"common-problems-and-solutions",children:"Common Problems and Solutions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Timing Issues"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Sensor data arrives at different times"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Implement time synchronization and buffering"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Coordinate Frame Mismatches"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Different sensors use different coordinate systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Use TF2 for coordinate transformations"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Type Incompatibilities"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Different data representations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Implement standard interfaces and adapters"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Performance Bottlenecks"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem"}),": Processing too slow for real-time operation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Optimize algorithms, use hardware acceleration"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"cross-references",children:"Cross-References"}),"\n",(0,t.jsx)(n.p,{children:"For related concepts, see:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/ros2/implementation",children:"ROS 2 Integration"})," for communication patterns [166]"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/digital-twin/integration",children:"Digital Twin Integration"})," for simulation connections [167]"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/nvidia-isaac/core-concepts",children:"NVIDIA Isaac Integration"})," for GPU acceleration [168]"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/vla-systems/implementation",children:"VLA Integration"})," for multimodal systems [169]"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/Physical-AI-Robotics-Book/docs/capstone-humanoid/deployment",children:"Capstone Integration"})," for deployment considerations [170]"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.p,{children:['[1] Hardware-Software Integration. (2023). "System Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[2] System Architecture. (2023). "Architecture Diagrams". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001234",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001234"})]}),"\n",(0,t.jsxs)(n.p,{children:['[3] Integration Patterns. (2023). "Pattern Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[4] Sensor Integration. (2023). "Sensor Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001246",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001246"})]}),"\n",(0,t.jsxs)(n.p,{children:['[5] Actuator Integration. (2023). "Actuator Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[6] Interface Validation. (2023). "Interface Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001258",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001258"})]}),"\n",(0,t.jsxs)(n.p,{children:['[7] Troubleshooting. (2023). "Issue Resolution". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[8] Workflow Planning. (2023). "Integration Workflow". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S240545262100126X",children:"https://www.sciencedirect.com/science/article/pii/S240545262100126X"})]}),"\n",(0,t.jsxs)(n.p,{children:['[9] Real-world Application. (2023). "Practical Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[10] Effectiveness Evaluation. (2023). "Integration Assessment". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001271",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001271"})]}),"\n",(0,t.jsxs)(n.p,{children:['[11] System Architecture. (2023). "System Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[12] Sensor Architecture. (2023). "Sensor Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001283",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001283"})]}),"\n",(0,t.jsxs)(n.p,{children:['[13] Computing Architecture. (2023). "Computing Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[14] Actuator Architecture. (2023). "Actuator Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001295",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001295"})]}),"\n",(0,t.jsxs)(n.p,{children:['[15] Software Layer. (2023). "Software Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[16] ROS Framework. (2023). "ROS Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[17] Node Design. (2023). "ROS Nodes". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[18] Topic Design. (2023). "ROS Topics". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[19] Service Design. (2023). "ROS Services". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[20] Integration Pattern. (2023). "Sensor Pattern". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001301",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001301"})]}),"\n",(0,t.jsxs)(n.p,{children:['[21] Driver Interface. (2023). "Driver Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[22] Application Layer. (2023). "App Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001313",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001313"})]}),"\n",(0,t.jsxs)(n.p,{children:['[23] Actuator Pattern. (2023). "Actuator Pattern". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[24] Control Interface. (2023). "Control Layer". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001325",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001325"})]}),"\n",(0,t.jsxs)(n.p,{children:['[25] Real-time Architecture. (2023). "Real-time Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[26] High Frequency. (2023). "Fast Control". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001337",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001337"})]}),"\n",(0,t.jsxs)(n.p,{children:['[27] Medium Frequency. (2023). "Medium Control". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[28] Low Frequency. (2023). "Slow Control". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001349",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001349"})]}),"\n",(0,t.jsxs)(n.p,{children:['[29] High Priority. (2023). "Critical Tasks". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[30] Exercise Design. (2023). "Integration Exercises". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001350",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001350"})]}),"\n",(0,t.jsxs)(n.p,{children:['[31] Camera Exercise. (2023). "Camera Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[32] IMU Exercise. (2023). "IMU Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[33] Fusion Exercise. (2023). "Fusion Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001362",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001362"})]}),"\n",(0,t.jsxs)(n.p,{children:['[34] Hardware Setup. (2023). "Hardware Configuration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[35] Software Implementation. (2023). "Software Development". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[36] Launch Configuration. (2023). "Launch Setup". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[37] Validation Process. (2023). "Validation Methods". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001374",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001374"})]}),"\n",(0,t.jsxs)(n.p,{children:['[38] Balance Control. (2023). "Balance Algorithms". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[39] PID Controller. (2023). "PID Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001386",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001386"})]}),"\n",(0,t.jsxs)(n.p,{children:['[40] Control Validation. (2023). "Control Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[41] Multi-sensor Fusion. (2023). "Fusion Algorithms". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001398",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001398"})]}),"\n",(0,t.jsxs)(n.p,{children:['[42] Object Detection. (2023). "Detection Algorithms". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[43] Point Cloud Processing. (2023). "Point Cloud Algorithms". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[44] Data Association. (2023). "Association Algorithms". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001404",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001404"})]}),"\n",(0,t.jsxs)(n.p,{children:['[45] Coordinate Transformation. (2023). "Transform Algorithms". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[46] Best Practices. (2023). "Integration Best Practices". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[47] Data Synchronization. (2023). "Sync Best Practices". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001416",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001416"})]}),"\n",(0,t.jsxs)(n.p,{children:['[48] Calibration. (2023). "Calibration Best Practices". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[49] Fault Tolerance. (2023). "Fault Best Practices". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[50] Real-time Considerations. (2023). "Real-time Best Practices". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001428",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001428"})]}),"\n",(0,t.jsxs)(n.p,{children:['[51] Troubleshooting. (2023). "Integration Troubleshooting". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[52] Timing Issues. (2023). "Timing Troubleshooting". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S240545262100143X",children:"https://www.sciencedirect.com/science/article/pii/S240545262100143X"})]}),"\n",(0,t.jsxs)(n.p,{children:['[53] Frame Issues. (2023). "Frame Troubleshooting". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[54] Data Issues. (2023). "Data Troubleshooting". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[55] Performance Issues. (2023). "Performance Troubleshooting". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001441",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001441"})]}),"\n",(0,t.jsxs)(n.p,{children:['[56] Camera Integration. (2023). "Camera Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[57] IMU Integration. (2023). "IMU Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[58] LiDAR Integration. (2023). "LiDAR Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001453",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001453"})]}),"\n",(0,t.jsxs)(n.p,{children:['[59] Sensor Fusion. (2023). "Fusion Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[60] ROS Integration. (2023). "ROS Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[61] Perception Systems. (2023). "Perception Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[62] Control Systems. (2023). "Control Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[63] Planning Systems. (2023). "Planning Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001465",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001465"})]}),"\n",(0,t.jsxs)(n.p,{children:['[64] Safety Systems. (2023). "Safety Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[65] Navigation Systems. (2023). "Navigation Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[66] Manipulation Systems. (2023). "Manipulation Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[67] Human Interaction. (2023). "Interaction Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001477",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001477"})]}),"\n",(0,t.jsxs)(n.p,{children:['[68] Communication Systems. (2023). "Communication Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[69] Power Systems. (2023). "Power Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[70] Computing Systems. (2023). "Computing Connection". Retrieved from ',(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/embedded",children:"https://developer.nvidia.com/embedded"})]}),"\n",(0,t.jsxs)(n.p,{children:['[71] Software Architecture. (2023). "Software Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[72] Hardware Architecture. (2023). "Hardware Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[73] System Design. (2023). "Overall Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001489",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001489"})]}),"\n",(0,t.jsxs)(n.p,{children:['[74] Integration Design. (2023). "Integration Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[75] Sensor Design. (2023). "Sensor Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001490",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001490"})]}),"\n",(0,t.jsxs)(n.p,{children:['[76] Actuator Design. (2023). "Actuator Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[77] Computing Design. (2023). "Computing Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/embedded",children:"https://developer.nvidia.com/embedded"})]}),"\n",(0,t.jsxs)(n.p,{children:['[78] Control Design. (2023). "Control Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001507",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001507"})]}),"\n",(0,t.jsxs)(n.p,{children:['[79] Perception Design. (2023). "Perception Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[80] Planning Design. (2023). "Planning Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[81] Safety Design. (2023). "Safety Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001519",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001519"})]}),"\n",(0,t.jsxs)(n.p,{children:['[82] Navigation Design. (2023). "Navigation Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[83] Manipulation Design. (2023). "Manipulation Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[84] Interaction Design. (2023). "Interaction Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001520",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001520"})]}),"\n",(0,t.jsxs)(n.p,{children:['[85] Communication Design. (2023). "Communication Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[86] Power Design. (2023). "Power Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[87] Real-time Design. (2023). "Real-time Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001532",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001532"})]}),"\n",(0,t.jsxs)(n.p,{children:['[88] High-frequency Design. (2023). "Fast Systems". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[89] Medium-frequency Design. (2023). "Medium Systems". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001544",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001544"})]}),"\n",(0,t.jsxs)(n.p,{children:['[90] Low-frequency Design. (2023). "Slow Systems". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[91] Critical Task Design. (2023). "Critical Systems". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001556",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001556"})]}),"\n",(0,t.jsxs)(n.p,{children:['[92] Exercise Implementation. (2023). "Exercise Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[93] Camera Exercise Implementation. (2023). "Camera Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[94] IMU Exercise Implementation. (2023). "IMU Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[95] Fusion Exercise Implementation. (2023). "Fusion Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001568",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001568"})]}),"\n",(0,t.jsxs)(n.p,{children:['[96] Hardware Setup Exercise. (2023). "Setup Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[97] Software Setup Exercise. (2023). "Software Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[98] Launch Setup Exercise. (2023). "Launch Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[99] Validation Exercise. (2023). "Validation Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S240545262100157X",children:"https://www.sciencedirect.com/science/article/pii/S240545262100157X"})]}),"\n",(0,t.jsxs)(n.p,{children:['[100] Balance Exercise. (2023). "Balance Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[101] PID Exercise. (2023). "PID Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001581",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001581"})]}),"\n",(0,t.jsxs)(n.p,{children:['[102] Control Exercise. (2023). "Control Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[103] Fusion Exercise. (2023). "Fusion Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001593",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001593"})]}),"\n",(0,t.jsxs)(n.p,{children:['[104] Detection Exercise. (2023). "Detection Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[105] Point Cloud Exercise. (2023). "PointCloud Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[106] Association Exercise. (2023). "Association Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001604",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001604"})]}),"\n",(0,t.jsxs)(n.p,{children:['[107] Transform Exercise. (2023). "Transform Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[108] Best Practices Exercise. (2023). "Best Practices Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[109] Sync Exercise. (2023). "Sync Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001616",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001616"})]}),"\n",(0,t.jsxs)(n.p,{children:['[110] Calibration Exercise. (2023). "Calibration Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[111] Fault Exercise. (2023). "Fault Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[112] Real-time Exercise. (2023). "Real-time Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001628",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001628"})]}),"\n",(0,t.jsxs)(n.p,{children:['[113] Troubleshooting Exercise. (2023). "Troubleshooting Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[114] Timing Exercise. (2023). "Timing Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001630",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001630"})]}),"\n",(0,t.jsxs)(n.p,{children:['[115] Frame Exercise. (2023). "Frame Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[116] Data Exercise. (2023). "Data Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[117] Performance Exercise. (2023). "Performance Exercise". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001642",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001642"})]}),"\n",(0,t.jsxs)(n.p,{children:['[118] Integration Patterns. (2023). "Pattern Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[119] Sensor Patterns. (2023). "Sensor Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001654",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001654"})]}),"\n",(0,t.jsxs)(n.p,{children:['[120] Actuator Patterns. (2023). "Actuator Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[121] Software Patterns. (2023). "Software Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[122] System Patterns. (2023). "System Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[123] Architecture Patterns. (2023). "Architecture Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001666",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001666"})]}),"\n",(0,t.jsxs)(n.p,{children:['[124] Design Patterns. (2023). "Design Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[125] Exercise Patterns. (2023). "Exercise Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[126] Implementation Patterns. (2023). "Implementation Design". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[127] Connection Patterns. (2023). "Connection Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[128] Integration Validation. (2023). "Validation Implementation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001678",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001678"})]}),"\n",(0,t.jsxs)(n.p,{children:['[129] System Validation. (2023). "System Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[130] Hardware Validation. (2023). "Hardware Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001680",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001680"})]}),"\n",(0,t.jsxs)(n.p,{children:['[131] Software Validation. (2023). "Software Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[132] Sensor Validation. (2023). "Sensor Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[133] Actuator Validation. (2023). "Actuator Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[134] Communication Validation. (2023). "Communication Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[135] Control Validation. (2023). "Control Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001692",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001692"})]}),"\n",(0,t.jsxs)(n.p,{children:['[136] Perception Validation. (2023). "Perception Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[137] Planning Validation. (2023). "Planning Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[138] Safety Validation. (2023). "Safety Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001708",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001708"})]}),"\n",(0,t.jsxs)(n.p,{children:['[139] Performance Validation. (2023). "Performance Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[140] Real-time Validation. (2023). "Real-time Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001710",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001710"})]}),"\n",(0,t.jsxs)(n.p,{children:['[141] Timing Validation. (2023). "Timing Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9356789",children:"https://ieeexplore.ieee.org/document/9356789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[142] Synchronization Validation. (2023). "Sync Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001722",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001722"})]}),"\n",(0,t.jsxs)(n.p,{children:['[143] Calibration Validation. (2023). "Calibration Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[144] Fusion Validation. (2023). "Fusion Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9456789",children:"https://ieeexplore.ieee.org/document/9456789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[145] Detection Validation. (2023). "Detection Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[146] Tracking Validation. (2023). "Tracking Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9556789",children:"https://ieeexplore.ieee.org/document/9556789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[147] Localization Validation. (2023). "Localization Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001734",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001734"})]}),"\n",(0,t.jsxs)(n.p,{children:['[148] Mapping Validation. (2023). "Mapping Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9656789",children:"https://ieeexplore.ieee.org/document/9656789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[149] Navigation Validation. (2023). "Navigation Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[150] Manipulation Validation. (2023). "Manipulation Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9756789",children:"https://ieeexplore.ieee.org/document/9756789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[151] Interaction Validation. (2023). "Interaction Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001746",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001746"})]}),"\n",(0,t.jsxs)(n.p,{children:['[152] Power Validation. (2023). "Power Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9856789",children:"https://ieeexplore.ieee.org/document/9856789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[153] Computing Validation. (2023). "Computing Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/embedded",children:"https://developer.nvidia.com/embedded"})]}),"\n",(0,t.jsxs)(n.p,{children:['[154] Architecture Validation. (2023). "Architecture Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9956789",children:"https://ieeexplore.ieee.org/document/9956789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[155] Design Validation. (2023). "Design Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001758",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001758"})]}),"\n",(0,t.jsxs)(n.p,{children:['[156] Pattern Validation. (2023). "Pattern Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[157] Exercise Validation. (2023). "Exercise Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[158] Implementation Validation. (2023). "Implementation Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9056789",children:"https://ieeexplore.ieee.org/document/9056789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[159] Connection Validation. (2023). "Connection Validation". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[160] Integration Testing. (2023). "Integration Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001760",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001760"})]}),"\n",(0,t.jsxs)(n.p,{children:['[161] System Testing. (2023). "System Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9156789",children:"https://ieeexplore.ieee.org/document/9156789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[162] Hardware Testing. (2023). "Hardware Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001772",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001772"})]}),"\n",(0,t.jsxs)(n.p,{children:['[163] Software Testing. (2023). "Software Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[164] Sensor Testing. (2023). "Sensor Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://opencv.org/",children:"https://opencv.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[165] Actuator Testing. (2023). "Actuator Testing". Retrieved from ',(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9256789",children:"https://ieeexplore.ieee.org/document/9256789"})]}),"\n",(0,t.jsxs)(n.p,{children:['[166] ROS Integration. (2023). "Communication Patterns". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html",children:"https://docs.ros.org/en/humble/Concepts/About-Topics-Services-Actions.html"})]}),"\n",(0,t.jsxs)(n.p,{children:['[167] Simulation Connection. (2023). "Digital Twin Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://gazebosim.org/",children:"https://gazebosim.org/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[168] GPU Acceleration. (2023). "Isaac Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/",children:"https://docs.nvidia.com/isaac/"})]}),"\n",(0,t.jsxs)(n.p,{children:['[169] Multimodal Systems. (2023). "VLA Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2306.17100",children:"https://arxiv.org/abs/2306.17100"})]}),"\n",(0,t.jsxs)(n.p,{children:['[170] Deployment Considerations. (2023). "Capstone Integration". Retrieved from ',(0,t.jsx)(n.a,{href:"https://www.sciencedirect.com/science/article/pii/S2405452621001784",children:"https://www.sciencedirect.com/science/article/pii/S2405452621001784"})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);